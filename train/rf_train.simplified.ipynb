{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "import pathlib\n",
    "import hashlib\n",
    "import json\n",
    "import sys\n",
    "import tempfile\n",
    "import urllib\n",
    "import os\n",
    "import collections\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "from numpy import inf\n",
    "import bz2\n",
    "import pickle\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "sns.set(font_scale= 1.0)\n",
    "import gc\n",
    "import time\n",
    "from skimpy import skim\n",
    "from pycaret.utils import *\n",
    "from pycaret.classification import *\n",
    "import natsort\n",
    "from natsort import *\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "print('pycaret',version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def not_missing_or_empty(fpath):\n",
    "    return os.path.exists(fpath) and os.path.isfile(fpath) and os.path.getsize(fpath) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_dict(d):\n",
    "    #sort dict according alphabetically to key\n",
    "    sorted_key = sorted(d.items(), key = lambda kv: kv[0])\n",
    "    return dict(sorted_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ioff()\n",
    "config_path='./config.json'\n",
    "config_dict={} \n",
    "if not_missing_or_empty(config_path):\n",
    "    print(f\"Configuration is taken from {config_path} \")\n",
    "    with open('./config.json','r') as f:\n",
    "        config_dict=json.load(f)\n",
    "else:\n",
    "    print(f\"Configuration is taken from program defaults\")\n",
    "    #pycaret parameters\n",
    "    config_dict['target']='KEY'\n",
    "    config_dict['session_id']=125\n",
    "    config_dict['verbose']=True\n",
    "    config_dict['normalize']=True\n",
    "    config_dict['normalize_method']='robust'\n",
    "    config_dict['fix_imbalance']=False\n",
    "    config_dict['fix_imbalance_method']=None\n",
    "    config_dict['feature_selection']=False\n",
    "    config_dict['feature_selection_method']='boruta'\n",
    "    config_dict['ignore_low_variance']=True\n",
    "    config_dict['remove_multicollinearity']=False\n",
    "    config_dict['remove_outliers']=False\n",
    "    config_dict['create_clusters']=False\n",
    "    config_dict['n_jobs']=10\n",
    "    config_dict['use_gpu']=False\n",
    "    config_dict['log_experiment']=False #When set to True, all metrics and parameters are logged on MLFlow server.\n",
    "    config_dict['experiment_name']=None #Name of experiment for logging. When set to None, ‘clf’ is by default used as alias for the experiment name.\n",
    "    config_dict['log_plots']=False #When set to True, specific plots are logged in MLflow as a png file. By default, it is set to False.\n",
    "    config_dict['log_profile']=False #When set to True, data profile is also logged on MLflow as a html file. By default, it is set to False.\n",
    "    config_dict['log_data']=False #When set to True, train and test dataset are logged as csv.\n",
    "    config_dict['fold_strategy']='stratifiedkfold' #'kfold', 'groupkfold', 'timeseries'\n",
    "    config_dict['data_split_stratify']=False\n",
    "    config_dict['fold']=10\n",
    "    #\n",
    "    config_dict['tune_baseline']=True \n",
    "    tune_dict={}\n",
    "    Metric='Accuracy'\n",
    "    config_dict['Metric']=Metric\n",
    "    tune_dict['default']=['50',''+Metric+'','scikit-learn','random','asha',True]\n",
    "    tune_dict['scikit_optimize']=['50',''+Metric+'','scikit-optimize','bayesian','asha',True]\n",
    "    tune_dict['sklearn_bayesian']=['50',''+Metric+'','tune-sklearn','bayesian','asha',True]\n",
    "    tune_dict['sklearn_hyperopt']=['50',''+Metric+'','tune-sklearn','hyperopt','asha',True]\n",
    "    tune_dict['sklearn_optuna']=['50',''+Metric+'','tune-sklearn','optuna','asha',True]\n",
    "    tune_dict['optuna']=['50',''+Metric+'','optuna','tpe','asha',True]\n",
    "    config_dict['tune_dict']=tune_dict\n",
    "    #\n",
    "    config_dict['plot_baseline']=True \n",
    "    plot_dict={}\n",
    "    plot_dict['error']=['Prediction Error','Class-Prediction-Error','.png',True] \n",
    "    plot_dict['boundary']=['Decision Boundary','Decision-Boundary','.png',True] \n",
    "    plot_dict['rfe']=['Recursive Feature Selection','Recursive-Feature-Selection','.png',False] \n",
    "    plot_dict['manifold']=['Manifold Learning','Manifold-Learning','.png',False] \n",
    "    plot_dict['calibration']=['Calibration Curve','Calibration-Curve','.png',False] \n",
    "    plot_dict['parameter']=['Model Hyperparameter','Model Hyperparameter','.png',False]\n",
    "    plot_dict['lift']=['Lift Curve','Lift-Curve','.png',False]\n",
    "    plot_dict['gain']=['Gain Chart','Gain-Chart','.png',False]\n",
    "    plot_dict['tree']=['Decision Tree','Decision-Tree','.png',False]\n",
    "    plot_dict['ks']=['KS Statistic Plot','KS-Statistic-Plot','.png',False]\n",
    "    plot_dict['class_report']=['Class Report','Classification-Report','.png',True]\n",
    "    plot_dict['auc']=['AUC','Area-Under-the-Curve','.png',True]\n",
    "    plot_dict['threshold']=['Discrimination Threshold','Discrimination-Threshold','.png',False]\n",
    "    plot_dict['confusion_matrix']=['Confusion Matrix','Confusion-Matrix','.png',True] \n",
    "    plot_dict['feature']=['Feature Importance','Feature-Importance','.png',True]\n",
    "    plot_dict['feature_all']=['Feature Importance (All)','Feature-Importance-All','.png',True]\n",
    "    plot_dict['learning']=['Learning Curve','Learning-Curve','.png',True] \n",
    "    plot_dict['pr']=['Precision Recall','Precision-Recall-Curve','.png',True] \n",
    "    plot_dict['vc']=['Validation Curve','Validation-Curve','.png',True]\n",
    "    config_dict['plot_dict']=plot_dict\n",
    "    #\n",
    "    config_dict['cm_cmap']='Blues'\n",
    "    #\n",
    "    config_dict['eda_plots']=False \n",
    "    config_dict['box_plot']=True\n",
    "    config_dict['violin_plot']=True\n",
    "    config_dict['count_plot']=True\n",
    "    config_dict['profile_plot']=True\n",
    "    config_dict['mean_profile_plot']=True\n",
    "    config_dict['feature_correlation_plot']=True\n",
    "    config_dict['set_dark_cs']=False\n",
    "    config_dict['umap_plot']=True\n",
    "    #\n",
    "    config_dict['LS_paper']=244\n",
    "    #\n",
    "    config_dict['model_name']='Random_Forrest'\n",
    "    config_dict['model']='rf'\n",
    "    config_dict['experiment_name_suffix']='auto'\n",
    "    #\n",
    "    config_dict['use_existing_feature_rejections']=True\n",
    "    config_dict['train_test_split']=False\n",
    "    config_dict['do_not_drop_in_group_duplicates_below_fill_up']=True\n",
    "    #\n",
    "    config_dict['fill_up']=100\n",
    "    config_dict['exact_fill_up']=False\n",
    "    config_dict['size_limit']=10000\n",
    "    config_dict['size_cut']=0\n",
    "    config_dict['n_trials']=0\n",
    "    #\n",
    "    config_dict['log2_transform']=False \n",
    "    config_dict['intergroup_drop']='' #value range: '','first','last',False  #(ad 'Most_Frequent')\n",
    "    config_dict['ingroup_drop']='' #value range: '','first','last',False\n",
    "    config_dict['size_limit_ingroup_drop']=config_dict['fill_up']\n",
    "    #\n",
    "    config_dict['store_train_and_test_seperately']=False\n",
    "    config_dict['stop_at_missing_nonsc']=True\n",
    "    config_dict['syscall_path']='./'\n",
    "    config_dict['h5_path']='./'\n",
    "    #\n",
    "    config_dict['l_channel']=['pr','pw','ma']  #channel: pr, pw, ma\n",
    "    config_dict['l_blur']=natsorted([0,4,12]) #mask: 0=unmasked,4=masked,12=DMA\n",
    "    config_dict['l_nfeat']=natsorted([512,256,192,128,96,64,48,32,24,16,8])\n",
    "    #\n",
    "    eval_default='./eval_file.h5'\n",
    "    test_file_dict={} \n",
    "    for channel in config_dict['l_channel']:\n",
    "        for blur in config_dict['l_blur']:\n",
    "            bv=str(channel)+'_'+str(blur)\n",
    "            test_file_dict[bv] =eval_default\n",
    "    config_dict['test_file_dict']=test_file_dict  \n",
    "    #\n",
    "    load_model_default='./final.model.pkl'\n",
    "    load_model_dict={} \n",
    "    for channel in config_dict['l_channel']:\n",
    "        for nfeat in config_dict['l_nfeat']:\n",
    "            for blur in config_dict['l_blur']:\n",
    "                bv=str(channel)+'_'+str(nfeat)+'_'+str(blur)\n",
    "                load_model_dict[bv]='./final.model.pkl'\n",
    "    config_dict['load_model_dict']=load_model_dict  \n",
    "    #\n",
    "    with open(config_path, 'w') as file:\n",
    "        json.dump(config_dict,file)\n",
    "        print(f\"{config_path} with program defaults has been generated\")\n",
    "#\n",
    "config_dict=sort_dict(config_dict)\n",
    "config_hash=hashlib.md5(open(config_path,'rb').read()).hexdigest()\n",
    "print(f\"Hash for {config_path} is {config_hash}\")\n",
    "for f in glob.iglob(\"./config.sorted.*\"):\n",
    "    if os.path.exists(f):\n",
    "        os.remove(f)\n",
    "with open(\"./config.sorted.\"+config_hash,'w') as file:\n",
    "    json.dump(config_dict,file)\n",
    "    print(f\"{file} with alphabetically sorted program defaults has been generated:\")\n",
    "    print(json.dumps(config_dict, sort_keys=True,indent=4, separators=(',', ': ')))\n",
    "#transform dictionary key, value to variable assignment key=value\n",
    "for key in config_dict:\n",
    "    if isinstance(config_dict[key],str):\n",
    "        exec(key+\"=\"+\"'\"+config_dict[key]+\"'\")\n",
    "    else:\n",
    "        exec(key+\"=\"+str(config_dict[key]))\n",
    "try:\n",
    "    experiment_name_suffix  # does a exist in the current namespace\n",
    "except NameError:\n",
    "    experiment_name_suffix = 'auto' # nope\n",
    "if experiment_name_suffix==\"auto\":\n",
    "    experiment_name_suffix=\".\"+config_hash\n",
    "print(f\"Experiment suffix for this calculation is {experiment_name_suffix}\")\n",
    "print(f\"Current working directory is {os.path.abspath(os.path.curdir)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logtransform an arbitrary matrix with non-positive entries\n",
    "#Calculate log to basis b of a Matrix allowing entries <= 0\n",
    "def logb(A,b=2):\n",
    "    lb=np.log10(b)\n",
    "    old_settings=np.seterr(divide='ignore', invalid='ignore')\n",
    "    P=np.log10(A)\n",
    "    P[P==-inf]=0\n",
    "    np.nan_to_num(P,copy=False)\n",
    "    if np.any(A<0):\n",
    "        N=np.log10(-A)\n",
    "        N[N==-inf]=0\n",
    "        np.nan_to_num(N,copy=False)\n",
    "        _=np.seterr(**old_settings)\n",
    "        A=(P-N)/lb\n",
    "    else:\n",
    "        A=P/lb\n",
    "    _=np.seterr(**old_settings)\n",
    "    return A\n",
    "def label_set(df,na):\n",
    "    Y=df['KEY'].to_numpy(copy=True)\n",
    "    S=natsorted(sorted(set(Y)))\n",
    "    print(len(S),\"train labels\",na,\"found:\")\n",
    "    print(S)\n",
    "    return S\n",
    "def calc_weights(df,na):\n",
    "    print(\"sample_weights\",na+\":\")\n",
    "    Y=pd.DataFrame(df['KEY'])\n",
    "    Y.reset_index(drop=True, inplace=True)\n",
    "    W=sklearn.utils.class_weight.compute_sample_weight(\"balanced\",Y,indices=None)\n",
    "    W=pd.DataFrame({'W': W})\n",
    "    W=pd.concat([Y,W],axis=1)\n",
    "    W.drop_duplicates(inplace=True,ignore_index=True)\n",
    "    W.sort_values(by=['KEY'],ascending=True,inplace=True)\n",
    "    W.to_csv(experiment_name+'.sample_weights.csv',index=0)\n",
    "    print('extrema of train weights:')\n",
    "    M=W['W'].max()\n",
    "    m=W['W'].min()\n",
    "    print(m,M)\n",
    "    bins,_=W.shape\n",
    "    binrange=\"(\"+str(int(m))+\",\"+str(int(M)+1)+\")\"\n",
    "    W.drop('KEY',axis=1).describe().T.style.bar(subset=['mean'], color='#205ff2')\\\n",
    "                        .background_gradient(subset=['std'], cmap='Reds')\\\n",
    "                        .background_gradient(subset=['50%'], cmap='coolwarm')\n",
    "    plt.rcParams[\"figure.figsize\"] = (10, 10)\n",
    "    sns.histplot(data=W['W'],bins=bins,binrange=(int(m),int(M)+1),kde=True,element=\"step\",stat='probability')\n",
    "    plt.plot()\n",
    "    plt.savefig(experiment_name+'.LABEL_WEIGHTS.png')\n",
    "    plt.close()\n",
    "    return dict(zip(W['KEY'],W['W']))\n",
    "def compare_label_sets(A,B,na,nb):\n",
    "   \"\"\"compare two label sets\n",
    "   Args:\n",
    "       A ([type]): [description]\n",
    "       B ([type]): [description]\n",
    "       na ([type]): [description]\n",
    "       nb ([type]): [description]\n",
    "   \"\"\"   \n",
    "   incons=0\n",
    "   if(len(A)>len(B)):\n",
    "      print('There are more '+na+' labels than '+nb+' labels')\n",
    "      incons=1\n",
    "   if(len(A)<len(B)):\n",
    "      print('There are less '+na+' labels than '+nb+' labels')\n",
    "   if(len(set(A)-set(B))>0):\n",
    "      print('The following '+na+' labels are not in the '+nb+' labels:')\n",
    "      print(natsorted(sorted(set(A)-set(B))))\n",
    "      incons=1\n",
    "   if(len(set(B)-set(A))>0):\n",
    "      print('The following '+nb+' labels are not in the '+na+' label set:')\n",
    "      print(natsorted(sorted(set(B)-set(A))))\n",
    "   if(len(set(B)&set(A))>0):\n",
    "      print('The following labels are in both label sets:')\n",
    "      print(natsorted(sorted(set(B)&set(A))))   \n",
    "   if(incons==0):\n",
    "       print(\"The \"+na+' label set is contained in the '+nb+' label set')\n",
    "   return\n",
    "def make_syscall_dicts(D):\n",
    "    \"\"\"Generate syscall dictionary from syscall file\n",
    "    Args:\n",
    "        D (pandas df): NR,SC csv-file\n",
    "    \"\"\" \n",
    "    nr2sc=pd.read_csv(D)\n",
    "    nr2sc.drop_duplicates(inplace=True)\n",
    "    nr2sc.reset_index(drop=True, inplace=True)\n",
    "    return dict(zip(nr2sc['NR'],nr2sc['SC'])),dict(zip(nr2sc['SC'],nr2sc['NR']))\n",
    "def Size_Profile(df):\n",
    "    size_profile=df[\"KEY\"].value_counts().to_frame()\n",
    "    size_profile.reset_index(level=0,inplace=True)\n",
    "    size_profile.columns=['ID','COUNT']\n",
    "    size_profile=pd.concat([size_profile['ID'].replace(nr2sc),size_profile],axis=1)\n",
    "    size_profile.columns=['NAME','ID','COUNT']\n",
    "    size_profile.sort_values(by=['COUNT'],ascending=True,inplace=True)\n",
    "    return size_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(syscall_path+'syscalls.csv') as f:\n",
    "    nr2sc,sc2nr=make_syscall_dicts(f)\n",
    "    with open('NR2SC.json', 'w') as file:\n",
    "        json.dump(nr2sc, file)\n",
    "    with open('SC2NR.json', 'w') as file:\n",
    "        json.dump(sc2nr, file)\n",
    "#start writing eval_dict\n",
    "#add <syscall nr> as column to eval_dict\n",
    "eval_dict={} \n",
    "eval_dict_columns=['NR']\n",
    "for key in sc2nr.keys():\n",
    "    eval_dict[key]=[sc2nr[key]]\n",
    "nr_set=set(nr2sc.keys())\n",
    "sc_set=set(sc2nr.keys())\n",
    "max_nr_sc=len(nr2sc)\n",
    "max_nr_sc_paper=314\n",
    "print('Size of current syscall dictionary         :',max_nr_sc)\n",
    "print('Size of         syscall dictionary in paper:',max_nr_sc_paper)\n",
    "print(\"\")\n",
    "with open(syscall_path+'SC-Missing.txt') as f:\n",
    "    missing_sc = [line.rstrip() for line in f]\n",
    "print(str(len(missing_sc))+' sycalls tagged as missing in paper:')\n",
    "#add <mentioned as missing in paper> column to eval_dict\n",
    "sc_set_not_covered=set(missing_sc)\n",
    "eval_dict_columns.append('P_MISS')\n",
    "for key in eval_dict.keys():\n",
    "    if key in missing_sc:\n",
    "        sc_set_not_covered.remove(key)\n",
    "        eval_dict[key].append(True)\n",
    "    else:\n",
    "        eval_dict[key].append(False)\n",
    "if sc_set_not_covered != set():\n",
    "    print('Warning: The following keys declared as missing in paper are not covered by basic key list:')\n",
    "    print(sc_set_not_covered)\n",
    "print(natsorted(missing_sc))\n",
    "missing_nr=[sc2nr[na] for na in missing_sc]\n",
    "print('Corresponding NR list')\n",
    "print(missing_nr)\n",
    "print(\"\")\n",
    "with open(syscall_path+'SC-Excluded.txt') as f:\n",
    "    excluded_sc = [line.rstrip() for line in f]\n",
    "excluded_sc_not_covered=set(excluded_sc)\n",
    "#add <scs excluded in paper> as column to eval_dict\n",
    "eval_dict_columns.append('P_EXCL')\n",
    "for key in eval_dict.keys():\n",
    "    if key in excluded_sc:\n",
    "        excluded_sc_not_covered.remove(key)\n",
    "        eval_dict[key].append(True)\n",
    "    else:\n",
    "        eval_dict[key].append(False)\n",
    "if excluded_sc_not_covered != set():\n",
    "    print('Warning: The following keys declared as excluded from paper are not covered by basic key list:')\n",
    "    print(excluded_sc_not_covered)\n",
    "print(str(len(excluded_sc))+' sycalls tagged as excluded in paper:')\n",
    "print(natsorted(excluded_sc))\n",
    "excluded_nr=[sc2nr[na] for na in excluded_sc]\n",
    "print('Corresponding NR list')\n",
    "print(excluded_nr)\n",
    "print(\"\")\n",
    "excluded=natsorted(set(excluded_nr)|set(missing_nr))\n",
    "print(\"In total, the following \"+str(len(excluded))+\" syscalls are tagged as \\\"to be skipped\\\" in the paper:\")\n",
    "print(excluded)\n",
    "print('')\n",
    "#\n",
    "try:\n",
    "    with open(syscall_path+'SC-Explicitly-Mentioned') as f:\n",
    "        explicitly_mentioned_sc = [line.rstrip() for line in f]\n",
    "except IOError:\n",
    "        print(IOError)\n",
    "if len(explicitly_mentioned_sc)==0:\n",
    "    print('Error:',syscall_path+'SC-Explicitly-Mentioned','is empty')\n",
    "explicitly_mentioned_sc_not_covered=set(explicitly_mentioned_sc)\n",
    "#add explicitly mentioned in paper column to eval_dict\n",
    "eval_dict_columns.append('P_Ment')\n",
    "for key in eval_dict.keys():\n",
    "    if key in explicitly_mentioned_sc:\n",
    "        explicitly_mentioned_sc_not_covered.remove(key)\n",
    "        eval_dict[key].append(True)\n",
    "    else:\n",
    "        eval_dict[key].append(False)\n",
    "if explicitly_mentioned_sc_not_covered != set():\n",
    "    print('Warning: The following keys explicitly mentioned in paper are not covered by basic key list:')\n",
    "    print(explicitly_mentioned_sc_not_covered)\n",
    "print(str(len(explicitly_mentioned_sc))+' sycalls explicitly shown in paper pictures:')\n",
    "print(natsorted(explicitly_mentioned_sc))\n",
    "explicitly_mentioned_nr=[sc2nr[na] for na in explicitly_mentioned_sc]\n",
    "print('Corresponding NR list')\n",
    "print(explicitly_mentioned_nr)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Balance_Groups(df,fill_up):\n",
    "    print(f\"Shape at start of group balancing {df.shape}\")\n",
    "    Shape_Book(df.shape[0],df.shape[1],shape_count,\"start group balancing\")\n",
    "    groupr=df.groupby('KEY')\n",
    "    #number of groups\n",
    "    C=len(groupr)\n",
    "#    for key,group in groupr:\n",
    "#        C+=1\n",
    "    if ingroup_drop != '': \n",
    "        li=[]\n",
    "        c=0\n",
    "        for key,group in groupr:\n",
    "            groupn=group.drop_duplicates(subset=None,ignore_index=True,keep=ingroup_drop)\n",
    "            if groupn.shape[0]<groupn.shape[0] and groupn.shape[0] > size_limit_ingroup_drop:\n",
    "                c+=1\n",
    "                li.append(groupn)\n",
    "            else:\n",
    "                li.append(group)\n",
    "        if c>0:\n",
    "            df=pd.concat(li,axis=0,ignore_index=True).sort_values(by=['KEY'],axis=0,ascending=True,ignore_index=True)\n",
    "            print(f\"Duplicates have been dropped from {c}/{C} groups\")\n",
    "            print(f\"Shape after in-group drop of key duplicates {df.shape}\")\n",
    "            Shape_Book(df.shape[0],df.shape[1],shape_count,\"after in-group drop of key duplicates\")\n",
    "        else:\n",
    "            print(\"No ingroup duplicates found\")\n",
    "    else:\n",
    "        print(\"No (potential) ingroup duplicates dropped\")\n",
    "    #\n",
    "    size_profile=Size_Profile(df)\n",
    "    NL=size_profile.nlargest(2,'COUNT')\n",
    "    lg_na=NL.iloc[0,0]\n",
    "    lg_co=NL.iloc[0,2]\n",
    "    nlg_na=NL.iloc[1,0]\n",
    "    nlg_co=NL.iloc[1,2]\n",
    "    li=[]\n",
    "    lg_co=min(size_limit,lg_co)\n",
    "    nlg_co=min(size_limit,nlg_co)\n",
    "    fill_up_list=[]\n",
    "    down=0\n",
    "    up=0\n",
    "    for key,group in groupr:\n",
    "        s=group.shape[0]\n",
    "        t=0\n",
    "        if s > size_limit:\n",
    "            t=1\n",
    "            down+=1\n",
    "            group=group.sample(n=size_limit,axis=0,ignore_index=True,random_state=42)\n",
    "            s=size_limit\n",
    "        if key==lg_na and s>fill_up and nlg_co<lg_co:\n",
    "            if t==0:\n",
    "                down+=1\n",
    "            if nlg_co>=fill_up:\n",
    "                group=group.sample(n=nlg_co,axis=0,ignore_index=True,random_state=42)\n",
    "            else:\n",
    "                group=group.sample(n=fill_up,axis=0,ignore_index=True,random_state=42)\n",
    "        if s < fill_up:\n",
    "            up+=1\n",
    "            fill_up_list.append(key)\n",
    "            N=int(fill_up/s)\n",
    "            R=fill_up-N*s\n",
    "            for n in range(N):\n",
    "                li.append(group)\n",
    "            if R > 0:\n",
    "                if exact_fill_up:\n",
    "                    li.append(group.sample(n=R,axis=0,ignore_index=True,random_state=42))\n",
    "                else:\n",
    "                    li.append(group)\n",
    "        li.append(group)\n",
    "    if down+up > 0:\n",
    "        df=pd.concat(li,axis=0,ignore_index=True).sort_values(by=['KEY'],axis=0,ascending=True,ignore_index=True)\n",
    "    if down > 0:\n",
    "        print(f\"{down}/{C} syscall groups have been down-sampled to group size {size_limit}\")       \n",
    "    else:\n",
    "        print(\"No syscall groups have been down-sampled\")\n",
    "    if len(fill_up_list)>0:\n",
    "        print(f\"The following {len(fill_up_list)}/{C} syscalls have been upsampled to group size >= {fill_up}:\")\n",
    "        print(set([ int(s)  for s in fill_up_list ]))\n",
    "        #print(set([ nr2sc[int(s)]  for s in fill_up_list ]))\n",
    "    else:\n",
    "        print(\"No syscall groups have been upsampled\")\n",
    "    print(f\"Shape after group balancing: {df.shape}\")\n",
    "    Shape_Book(df.shape[0],df.shape[1],shape_count,\"end group balancing\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Validation_Image(precision,learning,validation,featimp_all,featimp,confusion):\n",
    "    fig = plt.figure(figsize=(20, 30))\n",
    "    fig.tight_layout()\n",
    "    fig.suptitle('Validation Overview for '+experiment_name)\n",
    "    # setting values to rows and column variables\n",
    "    rows = 3\n",
    "    columns = 2\n",
    "    # Adds a subplot at the 1st position\n",
    "    fig.add_subplot(rows, columns, 1)\n",
    "    # showing image\n",
    "    if precision!=None and not_missing_or_empty(precision):\n",
    "        Image1 = mpimg.imread(precision)        \n",
    "        plt.imshow(Image1)\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Precision Recall\")\n",
    "    # Adds a subplot at the 2nd position\n",
    "    fig.add_subplot(rows, columns, 2)\n",
    "    # showing image\n",
    "    if learning!=None and not_missing_or_empty(learning):\n",
    "        Image2 = mpimg.imread(learning)\n",
    "        plt.imshow(Image2)\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Learning Curve\")\n",
    "    # Adds a subplot at the 3rd position\n",
    "    fig.add_subplot(rows, columns, 3)\n",
    "    # showing image\n",
    "    if validation!=None and not_missing_or_empty(validation):\n",
    "        Image3 = mpimg.imread(validation)\n",
    "        plt.imshow(Image3)\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Validation Curve\")\n",
    "    # Adds a subplot at the 4th position\n",
    "    fig.add_subplot(rows, columns, 4)\n",
    "    # showing image\n",
    "    if featimp_all!=None and not_missing_or_empty(featimp_all):\n",
    "        Image4 = mpimg.imread(featimp_all)\n",
    "        plt.imshow(Image4)\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Feature Importance (All)\")\n",
    "    # Adds a subplot at the 5th position\n",
    "    fig.add_subplot(rows, columns, 5)\n",
    "    if featimp!=None and not_missing_or_empty(featimp):\n",
    "        Image5 = mpimg.imread(featimp)      \n",
    "       # showing image\n",
    "        plt.imshow(Image5)\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Feature Importance\")\n",
    "    # Adds a subplot at the 6th position\n",
    "    fig.add_subplot(rows, columns, 6)\n",
    "    # showing image\n",
    "    if confusion!=None and not_missing_or_empty(confusion):\n",
    "        Image6 = mpimg.imread(confusion)\n",
    "        if version == '2.3.5':\n",
    "            plt.imshow(Image6[14000:,14150:])\n",
    "        else:\n",
    "            plt.imshow(Image6)\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Confusion Matrix\")\n",
    "        plt.savefig('Validation_Image.png')\n",
    "    plt.close('all') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Shape_Book(nrows,ncolumns,shape_count,text):\n",
    "    shape_count+=1\n",
    "    df_shape = pd.DataFrame([[channel,nfeat,blur,nrows,ncolumns,shape_count,text]] ,columns=['CHANNEL', 'NFEAT', 'BLUR','N_ROW','N_COL','SHAPE_COUNT','COMMENT'])\n",
    "    df_shape.to_csv(shape_na,mode='a') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Save_Config(yes,CNR2NR):\n",
    "    if not yes:\n",
    "        return\n",
    "    X_train=pycaret.classification.get_config('X_train')\n",
    "    y_train=pycaret.classification.get_config('y_train')\n",
    "    X_test=pycaret.classification.get_config('X_test')\n",
    "    y_test=pycaret.classification.get_config('y_test')\n",
    "    if store_train_and_test_seperately:\n",
    "        X_train.to_hdf(experiment_name+'.X_train.h5', key='OFF_SET',mode='w',complib='blosc:lz4',complevel=9)\n",
    "        y_train.to_hdf(experiment_name+'.y_train.h5', key='LABEL',mode='w',complib='blosc:lz4',complevel=9)\n",
    "        y_train_non_compact=y_train.replace(CNR2NR)\n",
    "        y_train_non_compact.to_hdf(experiment_name+'.y_train_non_compact.h5', key='LABEL',mode='w',complib='blosc:lz4',complevel=9)\n",
    "        X_test.to_hdf(experiment_name+'.X_test.h5', key='OFF_SET',mode='w',complib='blosc:lz4',complevel=9)\n",
    "        y_test.to_hdf(experiment_name+'.y_test.h5', key='LABEL',mode='w',complib='blosc:lz4',complevel=9)\n",
    "        y_test_non_compact=y_test.replace(CNR2NR)\n",
    "        y_test_non_compact.to_hdf(experiment_name+'.y_train_non_compact.h5', key='LABEL',mode='w',complib='blosc:lz4',complevel=9)\n",
    "    pycaret.classification.save_config(experiment_name+'.py_caret_setup_config.pkl')\n",
    "    Shape_Book(X_train.shape[0]+X_test.shape[0],X_train.shape[1]+1,shape_count,\"after pycaret preprocessing\")\n",
    "    return X_train,y_train,X_test,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Create_and_Save_CM(model,model_name,t):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    print(f'Explicit creation of CM using {model_name} on set {t} ...')\n",
    "    try:\n",
    "        y_predicted=predict_model(model)\n",
    "    except Exception as e:\n",
    "        print(f\"Prediction with {model_name} on set {t} failed: {e}\")\n",
    "        print(f'Explicit creation of CM using{model_name} on {t} failed')\n",
    "    else:\n",
    "        y_predicted.to_hdf(experiment_name+'.Y_PREDICTED.'+t+'.h5', key='Y_PREDICTED',mode='w',complib='blosc:lz4',complevel=9)\n",
    "        cm=confusion_matrix(y_predicted['KEY'],y_predicted['Label'],normalize='true')\n",
    "        print(cm)\n",
    "        F=np.zeros(cm.shape,dtype=bool)\n",
    "        T=np.ones(cm.shape,dtype=bool)\n",
    "        plt.rcParams[\"figure.figsize\"] = (30,30)\n",
    "        cm_cmap='Blues'\n",
    "        ax=sns.heatmap(cm,annot=False,cmap=cm_cmap)\n",
    "        ax.set_xticklabels(ax.get_xticklabels(),rotation = 90)\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('Truth')\n",
    "        plt.plot()\n",
    "        plt.savefig(experiment_name+'.CM.'+t+'.png')\n",
    "        plt.close()\n",
    "        pd.DataFrame(data=cm).to_hdf(experiment_name+'.CM.'+t+'.h5', key='CM',mode='w',complib='blosc:lz4',complevel=9)\n",
    "        print(f'Explicit creation of CM using {model_name} on {t} succeeded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Eda_Plots(DATA,CNR2NR):\n",
    "    #box_plot=False\n",
    "    if box_plot:\n",
    "        #box plot\n",
    "        tic = time.perf_counter()\n",
    "        print(f\"Box plot ...\")\n",
    "        fig, out_fig = plt.subplots(figsize = (15,60))\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.title(\"Box Plot of Train Matrix Features\", fontsize=22)\n",
    "        out_fig = sns.boxplot(data = DATA.drop('KEY', axis = 1), orient=\"h\", fliersize=1, palette='crest')\n",
    "        fig.savefig(experiment_name+'.BOX.png')\n",
    "        plt.close()\n",
    "        toc = time.perf_counter()\n",
    "        print(f\"Box plot took {toc - tic:0.4f} seconds\")\n",
    "    #violin_plot=False\n",
    "    if violin_plot:\n",
    "        #violin plot\n",
    "        tic = time.perf_counter()\n",
    "        print(f\"Violin plot ...\")\n",
    "        fig, out_fig = plt.subplots(figsize = (15,100))\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.title(\"Violoin Plot of Train Matrix Features\", fontsize=22)\n",
    "        out_fig = sns.violinplot(data = DATA.drop('KEY', axis = 1), orient=\"h\", fliersize=1, palette=\"crest\")\n",
    "        fig.savefig(experiment_name+'.VIOLIN.png')\n",
    "        plt.close()\n",
    "        toc = time.perf_counter()\n",
    "        print(f\"Violin plot took {toc - tic:0.4f} seconds\")\n",
    "    if count_plot:\n",
    "        # counting unique offsets per feature\n",
    "        tic = time.perf_counter()\n",
    "        print(f\"Unique offset count per feature ...\")\n",
    "        df_all = pd.concat([DATA.drop('KEY', axis = 1), DATA], axis = 0)\n",
    "        unique_df = pd.DataFrame(df_all.nunique()).reset_index()\n",
    "        unique_df.columns=['features','count']\n",
    "        fig, feat_bar = plt.subplots(figsize = (15,30))\n",
    "        plt.title(\"Unique Offset Count per feature\", fontsize=22)\n",
    "        feat_bar = sns.barplot(y=\"features\", x=\"count\", data = unique_df, palette=\"crest\", orient='h')\n",
    "        fig.savefig(experiment_name+'.COUNT.png')\n",
    "        plt.close('all')    \n",
    "        toc = time.perf_counter()\n",
    "        print(f\"Unique offsetcount per feature took {toc - tic:0.4f} seconds\")\n",
    "    if profile_plot:\n",
    "        #Distributions per label group\n",
    "        tic = time.perf_counter()\n",
    "        print(f\"Plotting distinct offset distributions per label grouped per feature ...\")\n",
    "        C=DATA.shape[1]\n",
    "        nc=5\n",
    "        R=int((C-1)/5)+1\n",
    "        fig, axes = plt.subplots(R,nc,figsize=(2*nc,2*(R-1)))\n",
    "        fig.supxlabel('Distinct Offset Distributions per Label grouped per Feature', ha='center', fontweight='bold')\n",
    "        target_order = sorted(DATA['KEY'].unique())\n",
    "        for idx, ax in zip(range(1,C), axes.flatten()):\n",
    "            cnt = DATA[f'MAD_{idx}'].value_counts().sort_index()\n",
    "            sns.kdeplot(x=f'MAD_{idx}', \n",
    "                hue='KEY', \n",
    "                hue_order=target_order,\n",
    "                data=DATA,\n",
    "                alpha=0.5, \n",
    "                linewidth=0.1, \n",
    "                fill=True,\n",
    "                thresh=0.05,\n",
    "                legend=False,\n",
    "                warn_singular=False,\n",
    "                cumulative=True,\n",
    "                log_scale=False,\n",
    "                ax=ax)\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            ax.set_xlabel('')\n",
    "            ax.set_ylabel('')\n",
    "            ax.spines['left'].set_visible(False)\n",
    "            cnt = len(DATA[f'MAD_{idx}'].unique())\n",
    "            ax.set_title(f'MAD_{idx}({cnt})', loc='right', weight='bold', fontsize=11)\n",
    "        #delete empty subplots in last row\n",
    "        d=0\n",
    "        for i in range(nc*R,C-1,-1):\n",
    "            d-=1\n",
    "            axes.flatten()[d].axis('off')    \n",
    "        fig.tight_layout()\n",
    "        fig.savefig(experiment_name+'.PROFILES.png')\n",
    "        plt.close()    \n",
    "        toc = time.perf_counter()\n",
    "        print(f\"Plot of distinct offset distributions per label grouped per feature took {toc - tic:0.4f} seconds\")\n",
    "    if mean_profile_plot:\n",
    "        #Mean offset profile per label group\n",
    "        tic = time.perf_counter()\n",
    "        print(f\"Plotting mean offset profile per label grouped per feature ...\")\n",
    "        C=DATA.shape[1]\n",
    "        fig, axes = plt.subplots(C, 1, figsize=(10, 2*C))\n",
    "        target_order = sorted(DATA['KEY'].unique())\n",
    "        mean = DATA.groupby('KEY').mean().sort_index()\n",
    "        std = DATA.groupby('KEY').std().sort_index()\n",
    "        for idx, ax in zip(range(1,C), axes.flatten()):\n",
    "            x = np.arange(len(mean[f'MAD_{idx}'].index))\n",
    "            ax.bar(x, mean[f'MAD_{idx}'],\n",
    "                yerr = std[f'MAD_{idx}'], \n",
    "                width=2,edgecolor=None)\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            ax.set_xlabel('')\n",
    "            ax.set_ylabel('')\n",
    "            ax.margins(0.1)\n",
    "            ax.spines['left'].set_visible(False)\n",
    "            ax.set_title(f'MAD_{idx}', loc='right', weight='bold', fontsize=11)\n",
    "        fig.supxlabel('Mean Offset Profile per Label grouped per feature', ha='center', fontweight='bold')\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(experiment_name+'.MEAN_PROFILES.png')\n",
    "        plt.close()    \n",
    "        toc = time.perf_counter()\n",
    "        print(f\"Plot of mean offset profile per label group took {toc - tic:0.4f} seconds\")\n",
    "    if feature_correlation_plot:\n",
    "        #feature correlation\n",
    "        tic = time.perf_counter()\n",
    "        print(f\"Plotting feature correlation matrix ...\")\n",
    "        C=DATA.shape[1]\n",
    "        fig, ax = plt.subplots(figsize=(0.2*C, 0.2*C))\n",
    "        corr = DATA.drop('KEY', axis = 1).corr()\n",
    "        mask = np.zeros_like(corr, dtype=bool)\n",
    "        mask[np.triu_indices_from(mask)] = True\n",
    "        sns.heatmap(corr,\n",
    "            square=True, center=0, linewidth=0.2,\n",
    "            cmap=sns.diverging_palette(240, 10, as_cmap=True),\n",
    "            mask=mask, ax=ax) \n",
    "        ax.set_title('Feature Correlation', loc='left', fontweight='bold')\n",
    "        fig.savefig(experiment_name+'.FEATURE_CORRELATION_MATRIX.png')\n",
    "        plt.close()    \n",
    "        toc = time.perf_counter()\n",
    "        print(f\"Plot of feature correlation matrix took {toc - tic:0.4f} seconds\")\n",
    "    #dark color scheme\n",
    "    from cycler import cycler   \n",
    "    raw_light_palette = [\n",
    "    (0, 122, 255), # Blue\n",
    "    (255, 149, 0), # Orange\n",
    "    (52, 199, 89), # Green\n",
    "    (255, 59, 48), # Red\n",
    "    (175, 82, 222),# Purple\n",
    "    (255, 45, 85), # Pink\n",
    "    (88, 86, 214), # Indigo\n",
    "    (90, 200, 250),# Teal\n",
    "    (255, 204, 0)  # Yellow\n",
    "    ]\n",
    "    raw_dark_palette = [\n",
    "    (10, 132, 255), # Blue\n",
    "    (255, 159, 10), # Orange\n",
    "    (48, 209, 88),  # Green\n",
    "    (255, 69, 58),  # Red\n",
    "    (191, 90, 242), # Purple\n",
    "    (94, 92, 230),  # Indigo\n",
    "    (255, 55, 95),  # Pink\n",
    "    (100, 210, 255),# Teal\n",
    "    (255, 214, 10)  # Yellow\n",
    "    ]\n",
    "    raw_gray_light_palette = [\n",
    "    (142, 142, 147),# Gray\n",
    "    (174, 174, 178),# Gray (2)\n",
    "    (199, 199, 204),# Gray (3)\n",
    "    (209, 209, 214),# Gray (4)\n",
    "    (229, 229, 234),# Gray (5)\n",
    "    (242, 242, 247),# Gray (6)\n",
    "    ]\n",
    "    raw_gray_dark_palette = [\n",
    "    (142, 142, 147),# Gray\n",
    "    (99, 99, 102),  # Gray (2)\n",
    "    (72, 72, 74),   # Gray (3)\n",
    "    (58, 58, 60),   # Gray (4)\n",
    "    (44, 44, 46),   # Gray (5)\n",
    "    (28, 28, 39),   # Gray (6)\n",
    "    ]\n",
    "    light_palette = np.array(raw_light_palette)/255\n",
    "    dark_palette = np.array(raw_dark_palette)/255\n",
    "    gray_light_palette = np.array(raw_gray_light_palette)/255\n",
    "    gray_dark_palette = np.array(raw_gray_dark_palette)/255\n",
    "    white_color = gray_light_palette[-2]\n",
    "    if set_dark_cs:\n",
    "        mpl.rcParams['axes.prop_cycle'] = cycler('color',dark_palette)\n",
    "        mpl.rcParams['figure.facecolor']  = gray_dark_palette[-2]\n",
    "        mpl.rcParams['figure.edgecolor']  = gray_dark_palette[-2]\n",
    "        mpl.rcParams['axes.facecolor'] =  gray_dark_palette[-2]\n",
    "        mpl.rcParams['text.color'] = white_color\n",
    "        mpl.rcParams['axes.labelcolor'] = white_color\n",
    "        mpl.rcParams['axes.edgecolor'] = white_color\n",
    "        mpl.rcParams['xtick.color'] = white_color\n",
    "        mpl.rcParams['ytick.color'] = white_color\n",
    "        mpl.rcParams['figure.dpi'] = 200\n",
    "        mpl.rcParams['axes.spines.top'] = False\n",
    "        mpl.rcParams['axes.spines.right'] = False\n",
    "    if umap_plot:\n",
    "        from umap import UMAP\n",
    "        #umap clustering\n",
    "        sample_size=100000\n",
    "        tic = time.perf_counter()\n",
    "        print(f\"UMAP clustering ...\")\n",
    "        if DATA.shape[0] <= sample_size:\n",
    "            DATA_sub=DATA\n",
    "        else:\n",
    "            DATA_sub = DATA.sample(sample_size, random_state=0)\n",
    "        target = DATA_sub['KEY'].astype('int')\n",
    "        umap = UMAP(random_state=0)\n",
    "        dr = umap.fit_transform(DATA_sub.iloc[:,:-1], target)\n",
    "        dr_data = np.vstack((dr.T, DATA_sub.KEY)).T\n",
    "        dr_df = pd.DataFrame(data=dr_data, columns=(\"D1\", \"D2\", \"KEY\"))\n",
    "        dr_df.to_hdf(experiment_name+'.UMAP_PLOT_DATA.h5', key='D1_D2_KEY',mode='w',complib='blosc:lz4',complevel=9)\n",
    "        toc = time.perf_counter()\n",
    "        print(f\"UMAP clustering took {toc - tic:0.4f} seconds\")\n",
    "        #Plotting\n",
    "        tic = time.perf_counter()\n",
    "        print(f\"Plotting 2D UMAP clusters ...\")\n",
    "        W=12\n",
    "        nj=10\n",
    "        Size=1\n",
    "        L=len(DATA_sub['KEY'].unique())\n",
    "        add_rows=int((L-1)/nj+1)\n",
    "        ni=nj+add_rows\n",
    "        H=W+(W-1)*int(add_rows/nj)\n",
    "        fig = plt.figure(figsize=(W,H*1.2))\n",
    "        gs = fig.add_gridspec(ni, nj)\n",
    "        ax = fig.add_subplot(gs[:-add_rows,:])\n",
    "        sub_axes=[None]*ni*nj\n",
    "        idx=-1\n",
    "        for i in range(-add_rows,0):\n",
    "            for j in range(nj):\n",
    "                idx=idx+1\n",
    "                if(idx==L):\n",
    "                    break\n",
    "                sub_axes[idx] = fig.add_subplot(gs[i,j])\n",
    "        for idx in range(L):\n",
    "            ax.scatter(x=dr[:,0][target==idx], y=dr[:,1][target==idx],s=Size, alpha=0.2)\n",
    "            for j in range(L):\n",
    "                sub_axes[j].scatter(x=dr[:,0][target==idx], y=dr[:,1][target==idx],\n",
    "                            s=Size if idx==j else 0.1, \n",
    "                            alpha = 0.4 if idx==j else 0.008, \n",
    "                            color = (dark_palette[j%9]) if idx==j else white_color,\n",
    "                            zorder=(idx==j)\n",
    "                           )   \n",
    "            sub_axes[idx].set_xticks([])\n",
    "            sub_axes[idx].set_yticks([])\n",
    "            sub_axes[idx].set_xlabel('')\n",
    "            sub_axes[idx].set_ylabel('')\n",
    "            #sub_axes[idx].set_title(f'SC_{idx+1}')\n",
    "            scna=nr2sc[CNR2NR[idx]][0:12]\n",
    "            print(f'{idx+1}: {scna} plotted')\n",
    "            sub_axes[idx].set_title(f'{scna}')\n",
    "            sub_axes[idx].spines['right'].set_visible(True)\n",
    "            sub_axes[idx].spines['top'].set_visible(True)  \n",
    "        ax.set_title('UMAP Offset Distribution (2D) ', fontweight='bold', fontfamily='serif', fontsize=20, loc='left')       \n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_xlabel('')\n",
    "        ax.set_ylabel('')\n",
    "        ax.spines['left'].set_visible(False)\n",
    "        ax.spines['bottom'].set_visible(False)\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(experiment_name+'.UMAP.png')\n",
    "        plt.close()    \n",
    "        toc = time.perf_counter()\n",
    "        print(f\"UMAP 2D cluster plot took {toc - tic:0.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_multiblock_data(sna,channel,nfeat,blur):\n",
    "        base_na=channel+\"_\"+str(nfeat)+\"_\"+str(blur)\n",
    "        tna=\"./\"+base_na+'_0.oneblock.h5'\n",
    "        T=tna.split('_')\n",
    "        with h5py.File(sna,'r') as hdf:\n",
    "            ls=list(hdf.keys())\n",
    "            non_scs=False\n",
    "            if not_missing_or_empty(tna):\n",
    "                os.remove(tna)\n",
    "                print(f\"Existing{tna} removed\")\n",
    "            G=0\n",
    "            for g in ls:\n",
    "                s=g.split('_')\n",
    "                if(len(s)!=10):\n",
    "                    continue\n",
    "                G+=1\n",
    "                if s[1]==str(65535):\n",
    "                    non_scs=True\n",
    "                df=pd.read_hdf(sna,key=g)\n",
    "                if df.shape[1]<nfeat:\n",
    "                    print(f'Warning: number of features in {sna} = {df.shape[1]} < {nfeat}')\n",
    "                    print(f'Warning: {sna} cannot be used for {channel}_{nefeat}_{blur}')\n",
    "                    df=pd.DataFrame()\n",
    "                    return df\n",
    "                else:\n",
    "                    df=df.iloc[:,0:nfeat]\n",
    "                df.to_hdf(tna,key='OFF_SET',mode='a',format='table',append=True,complib='blosc:lz4',complevel=9)\n",
    "                #print(f\"syscall group <{g}> added to {tna}\")\n",
    "            if not non_scs:\n",
    "                print(f'Warning: non-syscall group missing in {sna}')\n",
    "            df=pd.read_hdf(tna)\n",
    "            os.remove(tna)\n",
    "            return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_group_training(na,fna,experiment_name,size_cut,n_trials,use_existing_feature_rejections,load_model_path):\n",
    "    tic = time.perf_counter()\n",
    "    print(f\"Reading {fna} ...\")\n",
    "    try:\n",
    "        DATA=pd.read_hdf(fna)\n",
    "    except Exception as e:\n",
    "        if str(e).find('key must be provided when HDF5 file contains multiple datasets') ==-1:\n",
    "            print(e)\n",
    "        else:\n",
    "            DATA=read_multiblock_data(fna,channel,nfeat,blur)\n",
    "    if DATA.empty:\n",
    "        print(f'Warning: could not enter any data from {fna}')\n",
    "        return\n",
    "    toc = time.perf_counter()\n",
    "    print(f\"Reading {fna} took {toc - tic:0.4f} seconds\")\n",
    "    #add key in read-in data column to eval_dict\n",
    "    #\n",
    "    eval_dict_columns.append('H5_Input')\n",
    "    KEY_LIST=DATA['KEY'].unique()\n",
    "    KEY_LIST_not_covered=set(KEY_LIST)\n",
    "    for key in eval_dict.keys():\n",
    "        if eval_dict[key][0] in KEY_LIST:\n",
    "            KEY_LIST_not_covered.remove(eval_dict[key][0])\n",
    "            eval_dict[key].append(True)\n",
    "        else:\n",
    "            eval_dict[key].append(False)\n",
    "    if KEY_LIST_not_covered != set():\n",
    "        print('Warning: The following syscall indices occuring in input data are not covered by basic key list:')\n",
    "        print(KEY_LIST_not_covered)\n",
    "    #\n",
    "    print(\"\")\n",
    "    tic = time.perf_counter()\n",
    "    print(\"Balancing KEY groups ...\")\n",
    "    DATA = Balance_Groups(DATA,fill_up)\n",
    "    toc = time.perf_counter()\n",
    "    print(f\"Balancing KEY groups took {toc - tic:0.4f} seconds\")\n",
    "    size_profile=Size_Profile(DATA)\n",
    "    size_profile.to_csv(experiment_name+'.size_profile.csv',index=False)\n",
    "    print(\"Size profile:\")\n",
    "    print(size_profile)\n",
    "    print(\"\")\n",
    "    print(\"The 10 largest KEY groups are:\")\n",
    "    NL=size_profile.nlargest(10,'COUNT')\n",
    "    print(NL)\n",
    "    if train_test_split:\n",
    "        print(\"\")\n",
    "        tic=time.perf_counter()\n",
    "        print(\"Splitting into Train / Test data ...\")\n",
    "        y=DATA['KEY'].array\n",
    "        X_train,X_test,y_train,y_test=train_test_split(DATA.iloc[:,1:],y,stratify=y,test_size=0.25,random_state=42)\n",
    "        y_train=pd.DataFrame(data=y_train,columns=['KEY'])\n",
    "        X_train=X_train.reset_index(drop=True)\n",
    "        DATA=pd.concat([y_train,X_train],axis=1)\n",
    "        del(y_train)\n",
    "        del(X_train)\n",
    "        y_test=pd.DataFrame(data=y_test,columns=['KEY'])\n",
    "        X_test=X_test.reset_index(drop=True)\n",
    "        TEST=pd.concat([y_test,X_test],axis=1)\n",
    "        TEST.sort_values(by=['KEY'],ascending=True,inplace=True)\n",
    "        TEST.to_hdf(experiment_name+'.test.h5', key='OFF_SET',mode='w',complib='blosc:lz4',complevel=9)\n",
    "        del(y_test)\n",
    "        del(X_test)\n",
    "        del(TEST)\n",
    "        toc = time.perf_counter()\n",
    "        print(f\"Splitting took {toc - tic:0.4f} seconds\")\n",
    "    if intergroup_drop!=\"\":\n",
    "        print(\"\")\n",
    "        #drop inter-group duplicates from DATA\n",
    "        feat_list=DATA.columns.tolist()\n",
    "        feat_list.pop(0)\n",
    "        print(f\"Shape of train data before drop of inter-group duplicates {DATA.shape}\")\n",
    "        DATA=DATA.drop_duplicates(subset=feat_list,ignore_index=True,keep=intergroup_drop)\n",
    "        DATA=DATA.sort_values(by=['KEY'],axis=0,ascending=True,ignore_index=True)\n",
    "        DATA.sort_values(by=['KEY'],ascending=True,inplace=True)\n",
    "        print(f\"Shape of train data after  drop of inter-group duplicates {DATA.shape}\")\n",
    "        Shape_Book(DATA.shape[0],DATA.shape[1],shape_count,\"after inter-group drop of key duplicates\")\n",
    "        print(\"\")\n",
    "    else:\n",
    "        print(\"\")\n",
    "        print(\"Drop of inter-group duplicates has been switched off\")\n",
    "        print(\"\")\n",
    "    #\n",
    "    DATA['KEY'] = DATA['KEY'].apply(lambda x : -x if x < 0 else x)\n",
    "    #\n",
    "    tic = time.perf_counter()\n",
    "    print(\"Downcasting numeric types ...\")\n",
    "    fcols = DATA.select_dtypes('float').columns\n",
    "    icols = DATA.select_dtypes('integer').columns\n",
    "    DATA[fcols] = DATA[fcols].apply(pd.to_numeric, downcast='float')\n",
    "    DATA[icols] = DATA[icols].apply(pd.to_numeric, downcast='integer')\n",
    "    toc = time.perf_counter()\n",
    "    print(f\"Downcasting numeric types took {toc - tic:0.4f} seconds\")\n",
    "    #\n",
    "    print(DATA.info())\n",
    "    #\n",
    "    print(LS_paper,'train labels in the training data of the paper')  \n",
    "    LS=label_set(DATA,'in the current training data after duplicate drop')\n",
    "    if 65535 not in LS and 32768 not in LS:\n",
    "        if stop_at_missing_nonsc:\n",
    "            raise SystemExit('Error: No non-syscalls found.')\n",
    "        else:\n",
    "         print('Warning: no non-syscalls occur in dataset')\n",
    "    if len(LS)<LS_paper:\n",
    "        print('Warning: current training data contain smaller label set than in the paper')\n",
    "    Start_Label_Set=set(natsorted([nr2sc.get(i) for i in LS ]))\n",
    "    print(\"\")\n",
    "    print(\"syscall set at start:\")\n",
    "    print(Start_Label_Set)\n",
    "    print('The paper assumes that the following labels are missing in the available train data:')\n",
    "    print(natsorted(missing_nr))\n",
    "    compare_label_sets(missing_nr,LS,'missing','available train')\n",
    "    print(\"\")\n",
    "    print('The paper assumes that the following labels are excluded from the available train data:')\n",
    "    print(natsorted(excluded_nr))\n",
    "    compare_label_sets(excluded_nr,LS,'excluded','available train')\n",
    "    #\n",
    "    print(size_cut)\n",
    "    if size_cut=='paper':\n",
    "        if(len(excluded)>0):\n",
    "            DATA_S=DATA.loc[DATA['KEY'].isin(excluded) ]\n",
    "            LS_S=label_set(DATA_S,'exluded by paper') \n",
    "            #DATA_S.to_csv(experiment_name+'.DATA_S.csv.zip',index=0)\n",
    "            DATA_L=DATA.loc[~ DATA['KEY'].isin(excluded) ] \n",
    "        else:\n",
    "            os.system('rm -f DATA_S.csv.zip')\n",
    "            DATA_L=DATA\n",
    "            #DATA_L.to_csv(experiment_name+'.DATA_L.csv.zip',index=0)\n",
    "        LS_L=label_set(DATA_L,'not exluded by paper')\n",
    "    else:\n",
    "        if size_cut=='auto':\n",
    "            _,size_cut=DATA.shape\n",
    "            size_cut=size_cut-1\n",
    "            print('size cut is set to',size_cut)\n",
    "            if size_cut_upper_limit>0 and size_cut>size_cut_upper_limit:\n",
    "                size_cut=size_cut_upper_limit\n",
    "                print('size cut is limited to',size_cut_upper_limit,'in auto mode')\n",
    "        if size_cut > 0:\n",
    "            print('size cut is set to',size_cut)\n",
    "            DATA_S=DATA.groupby('KEY').filter(lambda x: len(x) < size_cut)\n",
    "            #DATA_S.to_csv(experiment_name+'.DATA_S.csv.zip',index=0)\n",
    "            LS_S=label_set(DATA_S,'below size '+str(size_cut))\n",
    "            DATA_L=DATA.groupby('KEY').filter(lambda x: len(x) >= size_cut)\n",
    "        else:\n",
    "            os.system('rm -f DATA_S.csv.zip')\n",
    "            DATA_L=DATA\n",
    "        #DATA_L.to_csv(experiment_name+'.DATA_L.csv.zip',index=0)\n",
    "        LS_L=label_set(DATA_L,'above size '+str(size_cut))\n",
    "    #downcasting DATA_L\n",
    "    fcols = DATA_L.select_dtypes('float').columns\n",
    "    icols = DATA_L.select_dtypes('integer').columns\n",
    "    DATA_L[fcols] = DATA_L[fcols].apply(pd.to_numeric, downcast='float')\n",
    "    DATA_L[icols] = DATA_L[icols].apply(pd.to_numeric, downcast='integer')\n",
    "    #\n",
    "    # minimum_paper_size_cut=smallest sample_size of all syscalls in explicitly_mentioned_sc\n",
    "    mask=(size_profile['NAME'].isin(explicitly_mentioned_sc))\n",
    "    size_profile_expl_ment=size_profile[mask]\n",
    "    print(size_profile_expl_ment.shape)\n",
    "    size_profile_expl_ment.sort_values(by=['COUNT'],ascending=True,inplace=False)\n",
    "    print(\"Size profile of SCs explicitly shown in paper pictures:\")\n",
    "    print(size_profile_expl_ment.T)  \n",
    "    min_consist_size_cut=size_profile_expl_ment['COUNT'].min()\n",
    "    if size_cut != 'paper':\n",
    "        if size_profile_expl_ment['COUNT'].min() < int(size_cut):\n",
    "            print(\"Warning: maximum size cut allowed to cover all SCs in paper pictures:\",min_consist_size_cut)\n",
    "            print('Warning: The following SCs explicitly shown in paper pictures are not covered by current size cut of',str(size_cut)+':')\n",
    "            not_covered=size_profile_expl_ment.mask(size_profile_expl_ment['COUNT'] >= size_cut)\n",
    "            not_covered.dropna(inplace=True)\n",
    "            not_covered=not_covered.astype({'ID': 'int32','COUNT': 'int32' })\n",
    "            print(not_covered)\n",
    "        else:\n",
    "            print(\"OK: maximum size cut covers all SCs in paper pictures\")\n",
    "    plt.rcParams[\"figure.figsize\"] = (15, 70)\n",
    "    if size_cut=='paper' and len(excluded)>0:\n",
    "        DATA_S[\"KEY\"].replace(nr2sc).value_counts().plot.barh(legend=None,title=\"Syscalls with < \"+str(size_cut)+\" rows\")\n",
    "        plt.plot()\n",
    "        plt.savefig(experiment_name+'.small_size_profile.png')\n",
    "        plt.close()\n",
    "    else:\n",
    "        if size_cut > 0:\n",
    "            DATA_S[\"KEY\"].replace(nr2sc).value_counts().plot.barh(legend=None,title=\"Syscalls with < \"+str(size_cut)+\" rows\")\n",
    "            plt.plot()\n",
    "            plt.savefig(experiment_name+'.small_size_profile.png')\n",
    "            plt.close()\n",
    "    DATA_L[\"KEY\"].replace(nr2sc).value_counts().plot.barh(legend=None,title=\"Syscalls with >= \"+str(size_cut)+\" rows\")\n",
    "    plt.plot()\n",
    "    plt.savefig(experiment_name+'.large_size_profile.png')\n",
    "    plt.close()\n",
    "    LS=label_set(DATA_L,'after drop of labels with row count <'+str(size_cut))\n",
    "    compare_label_sets(missing_nr,LS,'missing','available train')\n",
    "    print(\"\")\n",
    "    print('The paper assumes that the following labels are excluded from the available train data:')\n",
    "    print(natsorted(excluded_nr))\n",
    "    compare_label_sets(excluded_nr,LS,'excluded','available train')\n",
    "    del(DATA) \n",
    "    gc.collect()\n",
    "    class_weights=calc_weights(DATA_L,\"after size cut\")\n",
    "    LSW=natsorted([l for l in class_weights.keys()])\n",
    "    if LSW != LS:\n",
    "        print(\"\")\n",
    "        print('WARNING: LSW and LS differ')\n",
    "        print('LSW:')\n",
    "        print(LSW)\n",
    "        print('LS:')\n",
    "        print(LS)\n",
    "        compare_label_sets(LSW,LS,'weight','train')\n",
    "        print(\"\")\n",
    "    \n",
    "    # Tight packing of syscall NR due to cuml GPU requirements for some classifiers \n",
    "    #replace syscall keys by compact index list \"CNR\" starting from 0\n",
    "    NR2CNR=dict(zip(LS,[n for n in range(0,len(LS))]))\n",
    "    #add compact index to eval_dict\n",
    "    eval_dict_columns.append('CNR')\n",
    "    for key in eval_dict.keys():\n",
    "        eval_dict[key].append(NR2CNR.get(eval_dict[key][0]))\n",
    "    with open(experiment_name+'.NR2CNR.json', 'w') as file:\n",
    "        json.dump(dict(zip( [str(n) for n in LS], [n for n in range(0,len(LS))])), file)\n",
    "    print(\"NR -> CNR\")\n",
    "    print(NR2CNR)\n",
    "    CNR2NR=dict(zip([n for n in range(0,len(LS))],LS))\n",
    "    with open(experiment_name+'.CNR2NR.json', 'w') as file:\n",
    "        json.dump(dict(zip([n for n in range(0,len(LS))],[str(n) for n in LS])), file)\n",
    "    print(\"CNR -> NR\")\n",
    "    print(CNR2NR)    \n",
    "    #\n",
    "    #Final transformations required before training\n",
    "    #Labels transformed to [0,...,#labels-1] (required by use_gpu=True, but generally applied)\n",
    "    if log2_transform:\n",
    "        tic = time.perf_counter()\n",
    "        print(f\"Log2 transform of offset data ...\")\n",
    "        LOGX=pd.DataFrame(logb(DATA_L.iloc[:,1:].to_numpy(copy=True)),columns=DATA_L.iloc[:,1:].columns)\n",
    "        DATA_L.iloc[:,1:]=LOGX.apply(pd.to_numeric,downcast='float')\n",
    "        del(LOGX) \n",
    "        gc.collect()\n",
    "        print('log2 transform applied to offset data')\n",
    "        toc = time.perf_counter()\n",
    "        print(f\"Log2 transform of offset data took {toc - tic:0.4f} seconds\")\n",
    "    #\n",
    "    if eda_plots:\n",
    "        Eda_Plots(DATA_L,CNR2NR)\n",
    "    #\n",
    "    #Final adjustments\n",
    "    DATA_L[\"KEY\"].replace(to_replace=NR2CNR,inplace=True)\n",
    "    DATA_L.sort_values(by=['KEY'],inplace=True)\n",
    "    fcols = DATA_L.select_dtypes('float').columns\n",
    "    icols = DATA_L.select_dtypes('integer').columns\n",
    "    if use_gpu == True:\n",
    "        #transform int features to float (required for GPU processing)\n",
    "        DATA_L[icols]=DATA_L[icols].apply(pd.to_numeric,downcast='float')\n",
    "        DATA_L['KEY']=DATA_L['KEY'].astype('int')\n",
    "        labels=DATA_L['KEY'].unique()\n",
    "        print(labels)\n",
    "    DATA_L[fcols] = DATA_L[fcols].apply(pd.to_numeric, downcast='float')\n",
    "    DATA_L[icols] = DATA_L[icols].apply(pd.to_numeric, downcast='integer')\n",
    "    if 'KEY' not in icols:\n",
    "        DATA_L['KEY']=DATA_L['KEY'].astype('int')\n",
    "        DATA_L['KEY']=DATA_L['KEY'].apply(pd.to_numeric,downcast='integer')\n",
    "    #\n",
    "    print(\"Data structure immediatly before training:\")\n",
    "    skim(DATA_L)\n",
    "    #\n",
    "    if load_model_path != \"\":\n",
    "        print(f'Warning: model training is skipped due to existing model path {load_model_path}')\n",
    "        return load_model_path\n",
    "    #\n",
    "    # Pycaret Baseline\n",
    "    #\n",
    "    tic = time.perf_counter()\n",
    "    model_name='RandomForrestClassifier'\n",
    "    print(f\"Setting up pycaret model for {model_name} ...\")\n",
    "    experiment_name=experiment_name\n",
    "    n_jobs_default=int(config_dict['n_jobs'])\n",
    "    train_rf_baseline=setup(data=DATA_L,\n",
    "                    target=target, \n",
    "                    session_id=session_id, \n",
    "                    verbose=verbose, \n",
    "                    normalize=normalize,\n",
    "                    normalize_method=normalize_method,\n",
    "                    fix_imbalance=fix_imbalance,\n",
    "                    fix_imbalance_method=fix_imbalance_method,\n",
    "                    feature_selection=feature_selection,\n",
    "                    feature_selection_method=feature_selection_method,\n",
    "                    ignore_low_variance=ignore_low_variance, \n",
    "                    remove_multicollinearity=remove_multicollinearity,\n",
    "                    remove_outliers=remove_outliers,\n",
    "                    create_clusters=create_clusters,\n",
    "                    n_jobs=7,\n",
    "                    use_gpu=use_gpu,\n",
    "                    silent=True,\n",
    "                    log_experiment=False,\n",
    "                    experiment_name=experiment_name,\n",
    "                    log_plots=log_plots,\n",
    "                    log_profile=False,\n",
    "                    log_data=False,\n",
    "                    data_split_stratify=data_split_stratify,\n",
    "                    fold_strategy=fold_strategy,\n",
    "                    fold=fold\n",
    "                   )\n",
    "    toc = time.perf_counter()\n",
    "    print(f\"Pycaret model setup for {model_name} took {toc - tic:0.4f} seconds\")\n",
    "    #\n",
    "    del(DATA_L) \n",
    "    gc.collect()\n",
    "    #\n",
    "    Save_Config(True,CNR2NR)\n",
    "    #\n",
    "    eval_dict_columns.append('In_Train')\n",
    "    y_list=get_config('y_train').unique()\n",
    "    for key in eval_dict.keys():\n",
    "        if eval_dict[key][5] in y_list:\n",
    "            eval_dict[key].append(True)\n",
    "        else:\n",
    "            eval_dict[key].append(False)\n",
    "    #\n",
    "    tic = time.perf_counter()\n",
    "    print(f\"Creating model for {model_name} ...\")\n",
    "    pycaret.classification.set_config('n_jobs_param',3)\n",
    "    class_weight_method='balanced_subsample'\n",
    "    #class_weight_method='dictionary'\n",
    "    #\n",
    "    class_weights=calc_weights(pd.concat([get_config('y_train'),get_config('X_train')],axis=1),\"for train set\")\n",
    "    print(class_weights)\n",
    "    #add weights of training data column to eval_dict\n",
    "    eval_dict_columns.append('Train_Weight')\n",
    "    for key in eval_dict.keys():\n",
    "            eval_dict[key].append(class_weights.get(eval_dict[key][5]))\n",
    "    #\n",
    "    if use_gpu==False:\n",
    "        if class_weight_method=='balanced' or class_weight_method=='balanced_subsample':\n",
    "            rf = create_model('rf',class_weight=class_weight_method)\n",
    "        elif class_weight_method=='dictionary':\n",
    "            #calc_weights(DATA_TRAIN,\"after pycaret preprocessing\")\n",
    "            rf = create_model('rf',class_weight=class_weights)\n",
    "        else:\n",
    "            raise SystemExit('Error: unknown class weight method')            \n",
    "    else:\n",
    "        print('Warning: cannot use weights due to cuML restrictions')\n",
    "        rf = create_model('rf')\n",
    "        raise SystemExit('Error: will not continue')\n",
    "        #\n",
    "    results=pycaret.classification.pull()\n",
    "    results.to_csv(experiment_name+'.results.csv',index=0)\n",
    "    save_model(rf,experiment_name+\".model\")\n",
    "    toc = time.perf_counter()\n",
    "    print(f\"Model creation for {model_name} took {toc - tic:0.4f} seconds\")\n",
    "    #\n",
    "    file_path='./'+experiment_name+'.model.txt'\n",
    "    with open(file_path, \"w\") as o:\n",
    "        with contextlib.redirect_stdout(o):\n",
    "            print(rf)\n",
    "    #\n",
    "    Create_and_Save_CM(rf,model_name,'HOLD_OUT.BASE')\n",
    "    #\n",
    "    plot_failure=[]\n",
    "    img_dict={}\n",
    "    if plot_baseline:\n",
    "        model_name='RF_BASELINE'\n",
    "        print('------------------------------------------------------------------')\n",
    "        for key in plot_dict:\n",
    "            if not plot_dict[key][3]:\n",
    "                continue\n",
    "            if key == 'learning':\n",
    "                pycaret.classification.set_config('n_jobs_param',5)\n",
    "            if key == 'vc':\n",
    "                pycaret.classification.set_config('n_jobs_param',5)\n",
    "            sna=\"./'\"+plot_dict[key][0]+plot_dict[key][2]+\"'\"\n",
    "            tna=\"./\"+experiment_name+'.'+plot_dict[key][1]+plot_dict[key][2]\n",
    "            img_dict[key]=tna\n",
    "            print(sna+\" -> \"+tna+\" ...\")\n",
    "            try:\n",
    "                tic = time.perf_counter()\n",
    "                print(f\"Plotting <{plot_dict[key][0]}> for {model_name} ...\")\n",
    "                plot_model(rf,plot=key,save=True)\n",
    "            except Exception as e:\n",
    "                print(f\"Plotting <{plot_dict[key][0]}> for {model_name} caused the following error: {e}\")\n",
    "                plot_failure.append([channel,nfeat,blur,plot_dict[key][0],e])\n",
    "                if os.system('ls '+sna+' 2>/dev/null 1>/dev/null') == 0:\n",
    "                    os.system('mv '+sna+' '+tna)\n",
    "                    print(f\"{sna} could be saved despite error: {e}\")\n",
    "                else:\n",
    "                    print(f\"{sna} not found\")\n",
    "            else:\n",
    "                if os.system('ls '+sna+' 2>/dev/null') == 0:\n",
    "                    os.system('mv '+sna+' '+tna)\n",
    "                    print(f\"{sna} moved to {tna}\")\n",
    "                else:\n",
    "                    print(f\"{sna} not found even, though it should exist\")\n",
    "                    plot_failure.append([channel,nfeat,blur,plot_dict[key][0],sna+\" not found, even though it should exist\"])\n",
    "            pycaret.classification.set_config('n_jobs_param',int(n_jobs_default))\n",
    "            print('------------------------------------------------------------------')\n",
    "            print(\"\")\n",
    "        #\n",
    "        Validation_Image(img_dict.get('pr'),img_dict.get('learning'),img_dict.get('vc'),img_dict.get('feature'),img_dict.get('feature_all'),img_dict.get('confusion_matrix'))\n",
    "        if os.system('ls Validation_Image.png 2>/dev/null 1>/dev/null') == 0:\n",
    "            os.system('mv Validation_Image.png '+\"./\"+experiment_name+'.val_summary.png')\n",
    "        else:\n",
    "            print(\"Validation_Image not found even though it should exist\")\n",
    "            plot_failure.append([channel,nfeat,blur,plot_dict[key][0],\"Validation_Image not found even though it should exist\"])\n",
    "        if len(plot_failure)>0:\n",
    "            df_plot_failure = pd.DataFrame(plot_failure,columns=['CHANNEL', 'NFEAT', 'BLUR','PLOT_NAME','ERROR_MESSAGE'])\n",
    "            df_plot_failure.to_csv(plot_failure_na,mode='a')\n",
    "    #\n",
    "    pycaret.classification.set_config('n_jobs_param',int(n_jobs_default))\n",
    "    #\n",
    "    model='rf'\n",
    "    if tune_baseline:\n",
    "        tune_failure=[]\n",
    "        pycaret.classification.set_config('n_jobs_param',4)\n",
    "        for key,li in tune_dict.items():\n",
    "            if not li[5]:\n",
    "                continue\n",
    "            tic = time.perf_counter()\n",
    "            print(f\"Tuning {model_name} with {key} options ...\")\n",
    "            tune_model_na=model_name+'_tuned_'+key\n",
    "            tune_mod='rf_'+key\n",
    "            try:\n",
    "                print(f\"Tuning {model_name} with optimize={li[1]},search_library={li[2]},search_algorithm={li[3]},early_stopping={li[4]} ...\")\n",
    "                if key=='default':\n",
    "                    rf_default=tune_model(rf,n_iter=int(li[0]),optimize=li[1],search_library=li[2],search_algorithm=li[3],early_stopping=li[4],fit_kwargs={'class_weight':class_weights})\n",
    "                    print(rf_default)\n",
    "                    results_tuned=pycaret.classification.pull()\n",
    "                    results_tuned.to_csv(experiment_name+'.results.'+key+'.tuned.csv',index=0)\n",
    "                    file_path='./'+experiment_name+'.'+key+'.tuned.model.txt'\n",
    "                    with open(file_path, \"w\") as o:\n",
    "                        with contextlib.redirect_stdout(o):\n",
    "                            print(rf_default)\n",
    "                    save_model(rf_default,experiment_name+'.'+key+'.tuned.model')\n",
    "                elif key=='scikit_optimize':\n",
    "                    rf_scikit_optimize=tune_model(rf,n_iter=int(li[0]),optimize=li[1],search_library=li[2],search_algorithm=li[3],early_stopping=li[4])\n",
    "                    print(rf_scikit_optimize)\n",
    "                    results_tuned=pycaret.classification.pull()\n",
    "                    results_tuned.to_csv(experiment_name+'.results.'+key+'.tuned.csv',index=0)\n",
    "                    file_path='./'+experiment_name+'.'+key+'.tuned.model.txt'\n",
    "                    with open(file_path, \"w\") as o:\n",
    "                        with contextlib.redirect_stdout(o):\n",
    "                            print(rf_scikit_optimize)\n",
    "                    save_model(rf_scikit_optimize,experiment_name+'.'+key+'.tuned.model')\n",
    "                elif key=='sklearn_bayesian':\n",
    "                    rf_sklearn_bayesian=tune_model(rf,n_iter=int(li[0]),optimize=li[1],search_library=li[2],search_algorithm=li[3],early_stopping=li[4])\n",
    "                    print(rf_sklearn_bayesian)\n",
    "                    results_tuned=pycaret.classification.pull()\n",
    "                    results_tuned.to_csv(experiment_name+'.results.'+key+'.tuned.csv',index=0)\n",
    "                    file_path='./'+experiment_name+'.'+key+'.tuned.model.txt'\n",
    "                    with open(file_path, \"w\") as o:\n",
    "                        with contextlib.redirect_stdout(o):\n",
    "                            print(rf_sklearn_bayesian)\n",
    "                    save_model(rf_sklearn_bayesian,experiment_name+'.'+key+'.tuned.model')\n",
    "                elif key=='sklearn_hyperopt':\n",
    "                    rf_sklearn_hyperopt=tune_model(rf,n_iter=int(li[0]),optimize=li[1],search_library=li[2],search_algorithm=li[3],early_stopping=li[4])\n",
    "                    print(rf_sklearn_hyperopt)\n",
    "                    results_tuned=pycaret.classification.pull()\n",
    "                    results_tuned.to_csv(experiment_name+'.results.'+key+'.tuned.csv',index=0)\n",
    "                    file_path='./'+experiment_name+'.'+key+'.tuned.model.txt'\n",
    "                    with open(file_path, \"w\") as o:\n",
    "                        with contextlib.redirect_stdout(o):\n",
    "                            print(rf_sklearn_hyperopt)\n",
    "                    save_model(rf_sklearn_hyperopt,experiment_name+'.'+key+'.tuned.model')\n",
    "                elif key=='sklearn_optuna':\n",
    "                    rf_sklearn_optuna=tune_model(rf,n_iter=int(li[0]),optimize=li[1],search_library=li[2],search_algorithm=li[3],early_stopping=li[4])\n",
    "                    print(rf_sklearn_optuna)\n",
    "                    results_tuned=pycaret.classification.pull()\n",
    "                    results_tuned.to_csv(experiment_name+'.results.'+key+'.tuned.csv',index=0)\n",
    "                    file_path='./'+experiment_name+'.'+key+'.tuned.model.txt'\n",
    "                    with open(file_path, \"w\") as o:\n",
    "                        with contextlib.redirect_stdout(o):\n",
    "                            print(rf_sklearn_optuna)\n",
    "                    save_model(rf_sklearn_optuna,experiment_name+'.'+key+'.tuned.model')\n",
    "                elif key=='optuna':\n",
    "                    rf_optuna=tune_model(rf,n_iter=int(li[0]),optimize=li[1],search_library=li[2],search_algorithm=li[3],early_stopping=li[4])\n",
    "                    print(rf_optuna)\n",
    "                    results_tuned=pycaret.classification.pull()\n",
    "                    results_tuned.to_csv(experiment_name+'.results.'+key+'.tuned.csv',index=0)\n",
    "                    file_path='./'+experiment_name+'.'+key+'.tuned.model.txt'\n",
    "                    with open(file_path, \"w\") as o:\n",
    "                        with contextlib.redirect_stdout(o):\n",
    "                            print(rf_optuna)\n",
    "                    save_model(rf_optuna,experiment_name+'.'+key+'.tuned.model')\n",
    "                else:\n",
    "                    e=f\"Unknown tuning model {key}\"\n",
    "                    tune_failure.append([channel,nfeat,blur,li[1],li[2],li[3],e])\n",
    "                    print(f\"Error: Tuning {model_name} with optimize={li[1]},search_library={li[2]},search_algorithm={li[3]}] failed: {e}\")\n",
    "                    #exec(tune_mod+\"=tune_model(\"+str(model)+\",n_iter=\"+str(li[0])+\",optimize=\"+li[1]+\",search_library=\"+li[2]+\",search_algorithm=\"+li[3]+\",early_stopping=\"+li[4]+\")\")\n",
    "            except Exception as e:\n",
    "                tune_failure.append([channel,nfeat,blur,li[1],li[2],li[3],e])\n",
    "                print(f\"Error: Tuning {model_name} with optimize={li[1]},search_library={li[2]},search_algorithm={li[3]}] failed: {e}\")\n",
    "            toc = time.perf_counter()\n",
    "            print(f\"Tuning {model_name} with optimize={li[1]},search_library={li[2]},search_algorithm={li[3]}],early_stopping={li[4]} options took {toc - tic:0.4f} seconds\")\n",
    "            time.sleep(5)\n",
    "        #\n",
    "        if len(tune_failure)>0:\n",
    "            df_tune_failure = pd.DataFrame(tune_failure,columns=['CHANNEL', 'NFEAT', 'BLUR','OPT','LIB','SEARCH','ERROR_MESSAGE'])\n",
    "            df_tune_failure.to_csv(tune_failure_na,mode='a')\n",
    "    #\n",
    "    pycaret.classification.set_config('n_jobs_param',int(n_jobs_default))\n",
    "    #\n",
    "    tic = time.perf_counter()\n",
    "    print(f\"Creating the best model for {Metric} ...\")\n",
    "    best=pycaret.classification.automl(optimize='Accuracy')\n",
    "    print(best)\n",
    "    best_na='BEST_ACCURACY'\n",
    "    file_path='./'+experiment_name+'.'+best_na+'.model.txt'\n",
    "    with open(file_path, \"w\") as o:\n",
    "        with contextlib.redirect_stdout(o):\n",
    "            print(best)\n",
    "    save_model(best,experiment_name+'.'+best_na+'.model')\n",
    "    toc = time.perf_counter()\n",
    "    print(f\"Creation of the best model for {Metric} took {toc - tic:0.4f} seconds\")\n",
    "    pycaret.classification.set_config('n_jobs_param',2)\n",
    "    tic = time.perf_counter()\n",
    "    print(\"Finalizing the best model ...\")\n",
    "    final_rf = finalize_model(best)\n",
    "    save_model(final_rf,experiment_name+\".final.model\")\n",
    "    toc = time.perf_counter()\n",
    "    print(f\"Finalization of the best model took {toc - tic:0.4f} seconds\")\n",
    "    return experiment_name+\".final.model.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_evaluation(tana,eval_dict_columns,eval_dict,testdata_path):\n",
    "    if not not_missing_or_empty(tana):\n",
    "        print(f'Warning: the input file for final evaluation {tana} is empty or missing')\n",
    "        print('Warning: no final evaluation is performed')\n",
    "        return       \n",
    "    for key in eval_dict.keys():\n",
    "        eval_dict[key]=eval_dict[key][0:7]\n",
    "    eval_dict_columns=eval_dict_columns[0:7]\n",
    "    tic=time.perf_counter()\n",
    "    print(f'Reading {tana} ...')        \n",
    "    df=pd.read_hdf(tana)\n",
    "    #add <occurs in test data column> to eval_dict\n",
    "    key_list=df['KEY'].unique()\n",
    "    key_list_not_covered=set(key_list)\n",
    "    overwrite=\"\"\n",
    "    colna='In_Test'\n",
    "    if colna not in eval_dict_columns:\n",
    "        eval_dict_columns.append(colna)\n",
    "    else:\n",
    "        overwrite=eval_dict_columns.index(colna)\n",
    "    for key in eval_dict.keys():\n",
    "        if eval_dict[key][0] in key_list:\n",
    "            key_list_not_covered.remove(eval_dict[key][0])\n",
    "            if overwrite==\"\":\n",
    "                eval_dict[key].append(True)\n",
    "            else:\n",
    "                eval_dict[key][overwrite]=True \n",
    "        else:\n",
    "            if overwrite==\"\":\n",
    "                eval_dict[key].append(False)\n",
    "            else:\n",
    "                eval_dict[key][overwrite]=False                \n",
    "    if key_list_not_covered != set():\n",
    "        print('Warning: The following syscall indices occuring in the test data are not covered by basic key list:')\n",
    "        print(key_list_not_covered)\n",
    "    toc = time.perf_counter()\n",
    "    print(f'Reading {tana} took {toc - tic:0.4f} seconds')\n",
    "    #\n",
    "    #add <True Positive> column to eval_dict\n",
    "    tic = time.perf_counter()\n",
    "    print('Start calculating syscall metrics ....')\n",
    "    print(\"------------------------------------------------------------\")\n",
    "    print(\"Counting True Positives ...\")\n",
    "    TP=df['KEY'].value_counts()\n",
    "    key_list=df['KEY'].unique()\n",
    "    key_list_not_covered=set(key_list)\n",
    "    owi=\"\"\n",
    "    colna='TP'\n",
    "    if colna not in eval_dict_columns:\n",
    "        eval_dict_columns.append(colna)\n",
    "    else:\n",
    "        owi=eval_dict_columns.index(colna)\n",
    "    for key in eval_dict.keys():\n",
    "        i=eval_dict[key][0]\n",
    "        if i in key_list:\n",
    "            key_list_not_covered.remove(i)\n",
    "            if owi != \"\":\n",
    "                eval_dict[key][owi]=int(TP[i])\n",
    "                key_list_not_covered.remove(i)\n",
    "            else:\n",
    "                eval_dict[key].append(TP[i])\n",
    "    if key_list_not_covered != set():\n",
    "            for i in list(key_list_not_covered):\n",
    "                print(f'Warning: TP count {TP[i]} for syscall index {i} not added to evaluation dictionary')\n",
    "                print(f'Warning: {i} does not occur as index in the basic syscall list')\n",
    "    #\n",
    "    #add <True Negative> column to eval_dict\n",
    "    print(\"------------------------------------------------------------\")\n",
    "    print(\"Counting True Negatives ...\")\n",
    "    TN=-TP\n",
    "    TN=TN.add(len(df['KEY']))\n",
    "    key_list_not_covered=set(key_list)\n",
    "    owi=\"\"\n",
    "    colna='TN'\n",
    "    if colna not in eval_dict_columns:\n",
    "        eval_dict_columns.append(colna)\n",
    "    else:\n",
    "        owi=eval_dict_columns.index(colna)\n",
    "    for key in eval_dict.keys():\n",
    "        i=eval_dict[key][0]\n",
    "        if i in key_list:\n",
    "            key_list_not_covered.remove(i)\n",
    "            if owi != \"\":\n",
    "                eval_dict[key][owi]=TN[i]\n",
    "            else:\n",
    "                eval_dict[key].append(TN[i])\n",
    "    if key_list_not_covered != set():\n",
    "        for i in list(key_list_not_covered):\n",
    "            print(f'Warning: TN count {TN[i]} for syscall index {i} not added to evaluation dictionary')\n",
    "            print(f'Warning: {i} does not occur as index in the basic syscall list')\n",
    "    #\n",
    "    #add <False Negative> column to eval_dict\n",
    "    print(\"------------------------------------------------------------\")\n",
    "    print(\"Counting False Negatives ...\")\n",
    "    fig=plt.figure(figsize=(20, 20))\n",
    "    FN_dir='./FN_dir/'\n",
    "    pathlib.Path(FN_dir).mkdir(parents=True, exist_ok=True)\n",
    "    FN={}\n",
    "    total_fn_miss={}\n",
    "    fn_class_count={}\n",
    "    key_grouped=df.groupby('KEY')\n",
    "    for key,group in key_grouped:\n",
    "        total_fn_miss[key]=None\n",
    "        fn_class_count[key]=0\n",
    "        kc=group['Label'].value_counts()\n",
    "        tp=kc.get(key)\n",
    "        if tp == None:\n",
    "            print(f'Warning: {key} has no true positives')\n",
    "            FN[key]=group.shape[0]\n",
    "        else:\n",
    "            FN[key]=group.shape[0]-tp\n",
    "        if kc.count == 1 and kc[0]!=key:\n",
    "            print(f'Warning: {key} is completely mapped to false negative class {kc[0]}')\n",
    "            total_fn_miss[key]=kc[0]\n",
    "            fn_class_count[key]=1\n",
    "        else:\n",
    "            if kc.count()>1:\n",
    "                fn_class_count[key]=kc.count()-1\n",
    "                print(f'{key} has {fn_class_count[key]} false negative classes')\n",
    "                group[\"Label\"].value_counts().plot.bar(legend=None,title=f\"False negatives {key}\")\n",
    "                plt.plot()\n",
    "                plt.savefig(FN_dir+experiment_name+'.'+str(key)+'.FN_SPREAD.png')\n",
    "                plt.close('all')\n",
    "    for colna in ['FN', 'total_fn_miss', 'fn_class_count']:\n",
    "        key_list_not_covered=set(key_list)\n",
    "        owi=\"\"\n",
    "        if colna not in eval_dict_columns:\n",
    "            eval_dict_columns.append(colna)\n",
    "        else:\n",
    "            owi=eval_dict_columns.index(colna)\n",
    "        for key in eval_dict.keys():\n",
    "            i=eval_dict[key][0]\n",
    "            if i in key_list:\n",
    "                bv=eval(colna+'['+str(i)+']')\n",
    "                key_list_not_covered.remove(i)\n",
    "                if owi != \"\":\n",
    "                    eval_dict[key][owi]=bv\n",
    "                else:\n",
    "                    eval_dict[key].append(bv)\n",
    "        if key_list_not_covered != set():\n",
    "            for i in list(key_list_not_covered):\n",
    "                bv=eval(colna+'['+str(i)+']')\n",
    "                print(f'Warning: {colna} count {bv} for syscall index {i} not added to evaluation dictionary')\n",
    "                print(f'Warning: {i} does not occur as index in the basic syscall list')\n",
    "    #\n",
    "    #add <False Positive> column to eval_dict\n",
    "    print(\"------------------------------------------------------------\")\n",
    "    print(\"Counting False Positives ...\")\n",
    "    fig=plt.figure(figsize=(20, 20))\n",
    "    FP_dir='./FP_dir/'\n",
    "    pathlib.Path(FP_dir).mkdir(parents=True, exist_ok=True)\n",
    "    FP={}\n",
    "    total_fp_miss={}\n",
    "    fp_class_count={}\n",
    "    label_grouped=df.groupby('Label')\n",
    "    for label,group in label_grouped:\n",
    "        total_fp_miss[label]=None\n",
    "        fp_class_count[label]=0\n",
    "        lc=group['KEY'].value_counts()\n",
    "        tp=lc.get(label)\n",
    "        if tp == None:\n",
    "            print(f'Warning: {label} has no true positives')\n",
    "            FP[label]=group.shape[0]\n",
    "        else:\n",
    "            FP[label]=group.shape[0]-tp\n",
    "        if lc.count == 1 and lc[0]!=key:\n",
    "            print(f'Warning: {label} is completely mapped to false positive class {lc[0]}')\n",
    "            total_fp_miss[label]=lc[0]\n",
    "            fp_class_count[label]=1\n",
    "        else:\n",
    "            if lc.count()>1:\n",
    "                fp_class_count[label]=lc.count()-1\n",
    "                print(f'{label} has {fp_class_count[label]} false positive classes')\n",
    "                group[\"KEY\"].value_counts().plot.bar(legend=None,title=f\"False positives {label}\")\n",
    "                plt.plot()\n",
    "                plt.savefig(FP_dir+experiment_name+'.'+str(key)+'.FP_SPREAD.png')\n",
    "                plt.close('all')\n",
    "    key_list=df['Label'].unique()\n",
    "    for colna in ['FP', 'total_fp_miss', 'fp_class_count']:\n",
    "        key_list_not_covered=set(key_list)\n",
    "        owi=\"\"\n",
    "        print(eval_dict_columns)\n",
    "        if colna not in eval_dict_columns:\n",
    "            eval_dict_columns.append(colna)\n",
    "        else:\n",
    "            owi=eval_dict_columns.index(colna)\n",
    "        print(eval_dict_columns)\n",
    "        for key in eval_dict.keys():\n",
    "            i=eval_dict[key][0]\n",
    "            if i in key_list:\n",
    "                bv=eval(colna+'['+str(i)+']')\n",
    "                key_list_not_covered.remove(i)\n",
    "                if owi != \"\":\n",
    "                    eval_dict[key][owi]=bv\n",
    "                else:\n",
    "                    eval_dict[key].append(bv)\n",
    "        if key_list_not_covered != set():\n",
    "            for i in list(key_list_not_covered):\n",
    "                bv=eval(colna+'['+str(i)+']')\n",
    "                print(f'Warning: {colna} count {bv} for syscall index {i} not added to evaluation dictionary')\n",
    "                print(f'Warning: {i} does not occur as index in the basic syscall list')\n",
    "    #\n",
    "        eval_df=pd.DataFrame.from_dict(eval_dict, orient='index',columns=eval_dict_columns)\n",
    "        #Add metrics\n",
    "        eval_df['ACC']=(eval_df['TP']+eval_df['TN'])/(eval_df['TP']+eval_df['FP']+eval_df['FN']+eval_df['TN'])\n",
    "        eval_df['ACC']=eval_df['ACC'].round(3)\n",
    "        print('ACC added to eval_dict')\n",
    "        eval_df['ERR']=(eval_df['FP']+eval_df['FN'])/(eval_df['TP']+eval_df['FP']+eval_df['FN']+eval_df['TN'])\n",
    "        eval_df['ERR']=eval_df['ERR'].round(3)\n",
    "        print('ERR added to eval_dict')\n",
    "        eval_df['PRE']=eval_df['TP']/(eval_df['TP']+eval_df['FP'])\n",
    "        eval_df['PRE']=eval_df['PRE'].round(3)\n",
    "        print('PRE added to eval_dict')\n",
    "        eval_df['REC']=eval_df['TP']/(eval_df['TP']+eval_df['FN'])\n",
    "        eval_df['REC']=eval_df['REC'].round(3)\n",
    "        print('REC added to eval_dict')\n",
    "        eval_df['TNR']=eval_df['TN']/(eval_df['FP']+eval_df['TN'])\n",
    "        eval_df['TNR']=eval_df['TNR'].round(3)\n",
    "        print('TNR added to eval_dict')\n",
    "        eval_df['FPR']=eval_df['FP']/(eval_df['FP']+eval_df['TN'])\n",
    "        eval_df['FPR']=eval_df['FPR'].round(3)\n",
    "        print('FPR added to eval_dict')\n",
    "        eval_df['FNR']=eval_df['FN']/(eval_df['TP']+eval_df['FN'])\n",
    "        eval_df['FNR']=eval_df['FNR'].round(3)\n",
    "        print('FNR added to eval_dict')\n",
    "        eval_df['F1']=2*(eval_df['PRE']*eval_df['REC'])/(eval_df['PRE']+eval_df['REC'])\n",
    "        eval_df['F1']=eval_df['F1'].round(3)\n",
    "        print('F1 added to eval_dict')\n",
    "        Z=eval_df['TP']*eval_df['TN']-eval_df['FP']*eval_df['FN']\n",
    "        N=(eval_df['TP']+eval_df['FP'])*(eval_df['TP']+eval_df['FN'])*(eval_df['TN']+eval_df['FP'])*(eval_df['TN']+eval_df['FN'])\n",
    "        N=N.pow(0.5)                                                                                        \n",
    "        eval_df['MCC']=Z/N\n",
    "        eval_df['MCC']=eval_df['MCC'].round(3)\n",
    "        print('MCC added to eval_dict')\n",
    "        #Kappa = 2 * (TP * TN - FN * FP) / (TP * FN + TP * FP + 2 * TP * TN + FN^2 + FN * TN + FP^2 + FP * TN)\n",
    "        Z=2*(eval_df['TP']*eval_df['TN']-eval_df['FP']*eval_df['FN'])\n",
    "        N=eval_df['TP']*eval_df['FN']+eval_df['TP']*eval_df['FP']+2*eval_df['TP']*eval_df['TN']+eval_df['FN'].pow(2)+eval_df['FN']*eval_df['TN']+eval_df['FP'].pow(2)+eval_df['FP']*eval_df['TN']\n",
    "        eval_df['KAPPA']=Z/N\n",
    "        eval_df['KAPPA']=eval_df['KAPPA'].round(3)\n",
    "        print('KAPPA added to eval_dict')\n",
    "        #\n",
    "        eval_df=eval_df.convert_dtypes()\n",
    "        eval_df['MCC']=eval_df['MCC'].round(3)\n",
    "        eval_df.rename(columns={\"total_fn_miss\": \"FNTOTMIS\",\"total_fp_miss\": \"FPTOTMIS\",\"fn_class_count\": \"#FNCLAS\",\"fp_class_count\": \"#FPCLAS\"},inplace=True)\n",
    "        #\n",
    "        eval_df.to_csv(experiment_name+'.eval_df.csv')\n",
    "    results_dict= {}\n",
    "    print('generating sklearn class report ...')\n",
    "    class_report_dict=sklearn.metrics.classification_report(df['KEY'],df['Label'],output_dict=True,zero_division=0)\n",
    "    results_dict['accuracy']=[class_report_dict.pop('accuracy')]\n",
    "    for x in class_report_dict['macro avg'].keys():\n",
    "        results_dict['macro_avg'+'_'+x]=[class_report_dict['macro avg'][x]]  \n",
    "    for x in class_report_dict['weighted avg'].keys():\n",
    "        results_dict['weighted_avg'+'_'+x]=[class_report_dict['weighted avg'][x]]\n",
    "    results_dict['MCC']=[sklearn.metrics.matthews_corrcoef(df['KEY'],df['Label'])]\n",
    "    results_dict['KAPPA']=[sklearn.metrics.cohen_kappa_score(df['KEY'],df['Label'])] \n",
    "    class_report_df=pd.DataFrame.from_dict(class_report_dict,orient='index')\n",
    "    class_report_df['SCNA']=class_report_df.index\n",
    "    class_report_df['SCNA'] = pd.to_numeric(class_report_df['SCNA'],errors='coerce')\n",
    "    class_report_df['SCNA'].replace(nr2sc,inplace=True) \n",
    "    class_report_df.to_csv(experiment_name+'.final_class_report.csv')\n",
    "    print(sklearn.metrics.classification_report(df['KEY'],df['Label'],zero_division=0))\n",
    "    print(results_dict)\n",
    "    results_df=pd.DataFrame.from_dict(results_dict)\n",
    "    results_df.to_csv(experiment_name+'.final_result.csv')\n",
    "    tic = time.perf_counter()\n",
    "    print(f'Creating CM using the finialized RandomForrestClassifier and the unseen test data {testdata_path} ...')\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm=confusion_matrix(df['KEY'],df['Label'],normalize='true')\n",
    "    print(cm)   \n",
    "    F=np.zeros(cm.shape,dtype=bool)\n",
    "    T=np.ones(cm.shape,dtype=bool)\n",
    "    plt.rcParams[\"figure.figsize\"]=(30,30)\n",
    "    cm_cmap='Blues'\n",
    "    ax=sns.heatmap(cm,annot=False,cmap=cm_cmap)\n",
    "    #ax=sns.heatmap(cm,mask=np.where(cm==0,F,T),annot=False,cmap=cm_cmap)\n",
    "    ax.set_xticklabels(ax.get_xticklabels(),rotation = 90)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Truth')\n",
    "    plt.plot()\n",
    "    plt.savefig(experiment_name+'.CM.FINAL_TEST.png')\n",
    "    plt.close()\n",
    "    pd.DataFrame(data=cm).to_hdf(experiment_name+'.CM.FINAL_TEST.h5', key='CM',mode='w',complib='blosc:lz4',complevel=9)\n",
    "    toc = time.perf_counter()\n",
    "    print(f'Creation of CM using the finialized RandomForrestClassifier and the unseen test data {testdata_path} took {toc - tic:0.4f} seconds')\n",
    "    return eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_group_test(testdata_path,model_path,nfeat,reduce_number_of_test_features,accept_test_fna,CNR2NR,NR2SC,tana,load_model_path):\n",
    "    if not accept_test_fna:\n",
    "        print(f'{testdata_path} has not been accepted for test, see above')\n",
    "        return\n",
    "    if not not_missing_or_empty(model_path):\n",
    "        print(f'{model_path} is empty or missing')\n",
    "        print('Warning: no final evaluation will be performed')\n",
    "        return\n",
    "    if(load_model_path):\n",
    "        if load_model_path.endswith('.pkl'):\n",
    "            model_fna=load_model_path[:-len('.pkl')]\n",
    "        print(f'Loading model  {model_fna} ...')\n",
    "        model=load_model(model_fna)\n",
    "    else:\n",
    "        if model_path.endswith('.pkl'):\n",
    "            model_fna=model_path[:-len('.pkl')]\n",
    "        print(f'Loading model  {model_fna} ...')\n",
    "        model=load_model(model_fna)\n",
    "    print(f'{model_fna} loaded')\n",
    "    if not not_missing_or_empty(testdata_path):\n",
    "        print(f'{testdata_path} is empty or missing')\n",
    "        print('Warning: no final evaluation will be performed')\n",
    "        return\n",
    "    with h5py.File(testdata_path,'r') as hdf:\n",
    "        ls=list(hdf.keys())\n",
    "        non_scs=False\n",
    "        fail_dict={}\n",
    "        print(f'Generating {tana} ...')\n",
    "        if not_missing_or_empty(tana):\n",
    "            os.remove(tana)\n",
    "            print(f\"Existing {tana} removed\")\n",
    "        for g in ls:\n",
    "            s=g.split('_')\n",
    "            if(len(s)!=10):\n",
    "                continue\n",
    "            print('vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv')\n",
    "            if s[1]==str(65535):\n",
    "                non_scs=True\n",
    "            tic = time.perf_counter()\n",
    "            print(f\"Assessing prediction accuracy for group {g} ...\")\n",
    "            ticr = time.perf_counter()\n",
    "            print(f\"Reading group {g} ...\")\n",
    "            df=pd.read_hdf(testdata_path,key=g)\n",
    "            df=df.iloc[:,0:nfeat]\n",
    "            df.drop_duplicates(subset=None,ignore_index=True,inplace=True)\n",
    "            df.reset_index(inplace=True, drop=True)\n",
    "            df['KEY']=df['KEY'].astype(int)\n",
    "            tocr = time.perf_counter()\n",
    "            print(f\"Reading group {g} took {tocr - ticr:0.4f} seconds\")\n",
    "            #\n",
    "            ticp = time.perf_counter()\n",
    "            print(f'Predicting labels for group {g} ...')\n",
    "            y_predicted=predict_model(model,data=df)\n",
    "            df=y_predicted.filter(items=['KEY','Label','Score'])\n",
    "            print(df.info())\n",
    "            y_label=list(df['Label'])\n",
    "            y_label_set=set(y_label)\n",
    "            size_y_label_set=len(y_label_set)\n",
    "            print(f\"Size of the predicted label set for {s[1]} is {size_y_label_set}:\")\n",
    "            #transform y_label back to non-compact representation\n",
    "            y_label_not_compact=[] \n",
    "            for sci in y_label:\n",
    "                bv=CNR2NR.get(str(sci))\n",
    "                if bv==None:\n",
    "                    bv=65535\n",
    "                    CNR2NR[sci]=str(bv)\n",
    "                y_label_not_compact.append(int(bv))\n",
    "            y_label=y_label_not_compact.copy()\n",
    "            del(y_label_not_compact) \n",
    "            df['Label']=y_label\n",
    "            y_score=list(df['Score'])\n",
    "            y_key=list(df['KEY'])\n",
    "            y_key_set=set(y_key)\n",
    "            if len(y_key_set)!=1:\n",
    "                print(f'Warning: key set for {s[1]} does not contain exactly one element')\n",
    "                print(f\"Warning: size of the true label set for {s[1]} is {len(y_key_set)}:\")\n",
    "                #print(y_key_set)\n",
    "            else:\n",
    "                print(f'OK: true label set of group {g} contains exactly one element')\n",
    "                if int(list(y_key_set)[0]) != int(s[1]):\n",
    "                    print(f'Warning: true label set for {s[1]} contains {s[1]} as single element but should contain {list(y_key_set)[0]} instead')\n",
    "                else:\n",
    "                    print(f'OK: this single element is {int(s[1])}')\n",
    "            print('!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!')\n",
    "            print(g)\n",
    "            print(df)\n",
    "            df.to_hdf(tana,key='KEY_LABEL_SCORE',mode='a',format='table',append=True,complib='blosc:lz4',complevel=9)\n",
    "            del(df)\n",
    "            del(y_predicted)\n",
    "            #\n",
    "            tocp = time.perf_counter()\n",
    "            print(f\"Label prediction for group {g} took {tocp - ticp:0.4f} seconds\")\n",
    "            total=len(y_label)\n",
    "            success=len([i for i,j in zip(y_key,y_label) if i == j])\n",
    "            print(f'group {g}: {total-success}/{total} predictions failed')\n",
    "            toc = time.perf_counter()\n",
    "            print(f\"Assessment of prediction accuray for group {g} took {toc - tic:0.4f} seconds\")\n",
    "            print(\"\")\n",
    "    #\n",
    "    eval_df=final_evaluation(tana,eval_dict_columns,eval_dict,testdata_path)\n",
    "    return eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failure=[]\n",
    "success=[]\n",
    "missing=[]\n",
    "F=0\n",
    "#\n",
    "#Book keeping files\n",
    "success_na='./SUCCESS.'+config_hash+'.csv'\n",
    "if not_missing_or_empty(success_na):\n",
    "    os.remove(success_na)\n",
    "failure_na='./FAILURE.'+config_hash+'.csv'\n",
    "if not_missing_or_empty(failure_na):\n",
    "    os.remove(failure_na)\n",
    "plot_failure_na='./PLOT_FAILURE.'+config_hash+'.csv'\n",
    "#if not_missing_or_empty(plot_failure_na):\n",
    "#    os.remove(plot_failure_na)\n",
    "if os.path.exists(plot_failure_na):\n",
    "        sh.rm('f', plot_failure_na)\n",
    "tune_failure_na='./TUNE_FAILURE.'+config_hash+'.csv'\n",
    "if not_missing_or_empty(tune_failure_na):\n",
    "    os.remove(tune_failure_na)\n",
    "missing_na='./MISSING.'+config_hash+'.csv'\n",
    "if not_missing_or_empty(missing_na):\n",
    "    os.remove(missing_na)\n",
    "shape_na='./SHAPE_LOG.'+config_hash+'.csv'\n",
    "if not_missing_or_empty(shape_na):\n",
    "    os.remove(shape_na)\n",
    "#\n",
    "TIC = time.perf_counter()\n",
    "#\n",
    "for channel in l_channel:\n",
    "    for blur in l_blur:\n",
    "        test_fna=''\n",
    "        test_nfeat_set=set()\n",
    "        test_nfeat=0\n",
    "        bv=str(channel)+'_'+str(blur)\n",
    "        if test_file_dict.get(bv) != None:\n",
    "            test_fna=os.path.realpath(test_file_dict[bv])\n",
    "            if not_missing_or_empty(test_fna):\n",
    "                print(f'{test_fna} contains test data for channel={channel} and blur={blur}')\n",
    "                #determine test_dict\n",
    "                test_shape_dict={}\n",
    "                print(f\"Analyzing {test_fna} ...\")\n",
    "                tic = time.perf_counter()\n",
    "                with h5py.File(test_fna,'r') as hdf:\n",
    "                    ls=list(hdf.keys())\n",
    "                    non_scs=False\n",
    "                    for g in ls:\n",
    "                        s=g.split('_')\n",
    "                        if(len(s)!=10):\n",
    "                            continue\n",
    "                        if s[1]==str(65535):\n",
    "                            non_scs=True\n",
    "                        c=0\n",
    "                        for dset in hdf[g].keys():\n",
    "                            if c==1:\n",
    "                                arr=hdf[g][dset] \n",
    "                                x=arr.shape\n",
    "                                x=x[0] \n",
    "                                exec('y='+str(hdf[g][dset].dtype[-1]))\n",
    "                                y=y[1][0]\n",
    "                                if y==1:\n",
    "                                    exec('y='+str(hdf[g][dset].dtype[-2]))                                    \n",
    "                                test_shape_dict[g]=(x,y)\n",
    "                            c+=1\n",
    "                        print(g,\":\",test_shape_dict[g])\n",
    "                        #print(g,\":\",test_shape_dict[g],df.duplicated().sum())\n",
    "                    if len(test_shape_dict)==0:\n",
    "                            print(f'{test_fna} has no identifiable syscall blocks')\n",
    "                            test_fna=\"\"\n",
    "                    else:\n",
    "                        if non_scs==False:\n",
    "                            print(f'Warning: {test_fna} contains no non-syscalls')\n",
    "                        for g in test_shape_dict.keys():\n",
    "                            test_nfeat_set.add(test_shape_dict[g][1])\n",
    "                        if len(test_nfeat_set)!=1:\n",
    "                            print(f'Warning: The syscall goups in {test_fna} have not all the same nfeat value ')\n",
    "                            print(f'{test_fna} is rejected as test file')\n",
    "                            test_fna=\"\"\n",
    "                        else:\n",
    "                            test_nfeat=list(test_nfeat_set)\n",
    "                            test_nfeat=test_nfeat[-1]+1\n",
    "                            print(f'{test_fna} has {test_nfeat} features and {len(test_shape_dict)} syscall blocks  ')\n",
    "                    toc = time.perf_counter()\n",
    "                    print(f\"Analysis of {test_fna}  took {toc - tic:0.4f} seconds\")\n",
    "            else:\n",
    "                print(f'Warning: {test_fna} provided as test file for channel={channel} and blur={blur} is empty or missing')\n",
    "                print(f'Warning: No extra test data are available for all nfeats of channel={channel} and blur={blur}')\n",
    "                test_fna=''\n",
    "        for nfeat in l_nfeat:\n",
    "            ticc = time.perf_counter()\n",
    "            accept_test_fna=False\n",
    "            reduce_number_of_test_features=False\n",
    "            if test_fna!='':\n",
    "                if test_nfeat==0:\n",
    "                    print(f'Error: test_nfeat should be > 0 at this stage, but is not')\n",
    "                else:\n",
    "                    if nfeat > test_nfeat:\n",
    "                        print(f'Warning: train nfeat={nfeat} > test nfeat={test_nfeat}')\n",
    "                        print(f'Warning: cannot use {test_fna}')\n",
    "                    else:\n",
    "                        accept_test_fna=True\n",
    "                        if nfeat < test_nfeat:\n",
    "                             reduce_number_of_test_features=True \n",
    "            shape_count=0\n",
    "            print(\"------------------------------------------------------------\")\n",
    "            F+=1\n",
    "            na=channel+\"_\"+str(nfeat)+\"_\"+str(blur)\n",
    "            fna=h5_path+na+\"_0.h5\"\n",
    "            experiment_name=na+'_'+model_name+experiment_name_suffix\n",
    "            if not_missing_or_empty(fna):\n",
    "                load_model_path=\"\"\n",
    "                if load_model_dict.get(na) != None:\n",
    "                    load_model_path=os.path.realpath(load_model_dict.get(na))\n",
    "                    if not not_missing_or_empty(load_model_path):\n",
    "                        print(f'Warning: {load_model_path} empty or missing')\n",
    "                        load_model_path=\"\"\n",
    "                if load_model_path==\"\":\n",
    "                    print(f\"Training {model_name} for group {na} ...\")\n",
    "                else:\n",
    "                    print(f'Will skip model training and load existing model path {load_model_path} instead')\n",
    "                try:\n",
    "                    ticr = time.perf_counter()\n",
    "                    print(f'Training model for group {na} ...')\n",
    "                    path_to_final_model=run_group_training(na,fna,experiment_name,size_cut,n_trials,use_existing_feature_rejections,load_model_path)\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    failure.append([channel,nfeat,blur,e])\n",
    "                    df_failure = pd.DataFrame(failure,columns=['CHANNEL', 'NFEAT', 'BLUR','ERROR_MESSAGE'])\n",
    "                    df_failure.to_csv(failure_na)\n",
    "                    print(f\"Training of {model_name} for group {na} failed\")\n",
    "                    tocr = time.perf_counter()\n",
    "                    print(f\"Training for group {na} took {tocr - ticr:0.4f} seconds\")\n",
    "                else:\n",
    "                    success.append([channel,nfeat,blur] )\n",
    "                    df_success = pd.DataFrame(success,columns=['CHANNEL', 'NFEAT', 'BLUR'])\n",
    "                    df_success.to_csv(success_na)\n",
    "                    print(f\"Training of {model_name} for group {na} succeeded\")\n",
    "                    tocr = time.perf_counter()\n",
    "                    print(f\"Training for group {na} took {tocr - ticr:0.4f} seconds\")\n",
    "                    #\n",
    "                    allow_final_test_evaluation=True\n",
    "                    if allow_final_test_evaluation:\n",
    "                        if test_fna!=\"\":\n",
    "                            if path_to_final_model=='':\n",
    "                                print(f'Warning: skipping final test for {test_fna}, since no path to finalized modell is available')\n",
    "                            elif not not_missing_or_empty(path_to_final_model):\n",
    "                                print(f'Warning: skipping final test for {test_fna}, finalized model {path_to_final_model} is empty or missing')\n",
    "                            else:\n",
    "                                tict = time.perf_counter()\n",
    "                                print(f'Running final test for {path_to_final_model} with test data from {test_fna} ...')\n",
    "                                CNR2NR_NA=experiment_name+'.CNR2NR.json'\n",
    "                                NR2SC_NA='./NR2SC.json'\n",
    "                                if not not_missing_or_empty(CNR2NR_NA):\n",
    "                                    print(f'{CNR2NR_NA} empty or missing')\n",
    "                                    print(f'Warning: CNR2NR dictionary required for back transforming model syscall indices to non-compact form')\n",
    "                                    print(f'Warning: test of group {na} will be skipped')\n",
    "                                elif not not_missing_or_empty(NR2SC_NA):\n",
    "                                    print(f'{NR2SC_NA} empty or missing')\n",
    "                                    print(f'Warning: NR2SC dictionary required for back transforming model syscall indices to syscall names')\n",
    "                                    print(f'Warning: test of group {na} will be skipped')\n",
    "                                else:\n",
    "                                    CNR2NR={}\n",
    "                                    NR2SC={}\n",
    "                                    with open(CNR2NR_NA) as f: \n",
    "                                        CNR2NR=json.load(f)\n",
    "                                    with open(NR2SC_NA) as f: \n",
    "                                       NR2SC=json.load(f)\n",
    "                                    tana=experiment_name+'.KEY_LABEL_SCORE.h5'\n",
    "                                    run_group_test(test_fna,path_to_final_model,nfeat,reduce_number_of_test_features,accept_test_fna,CNR2NR,NR2SC,tana,load_model_path)\n",
    "                                    toct = time.perf_counter()\n",
    "                                    print(f\"Final test for group {na}: {toct - tict:0.4f} seconds\")\n",
    "                        else: \n",
    "                            print('No final test file available')\n",
    "                            print('Warning: final test evaluation is skipped')\n",
    "                    else:\n",
    "                        print('Final test evaluation switch off by configuration')\n",
    "            else:\n",
    "                missing.append([channel,nfeat,blur] )\n",
    "                df_missing = pd.DataFrame(missing,columns=['CHANNEL', 'NFEAT', 'BLUR'])\n",
    "                df_missing.to_csv(missing_na)\n",
    "                print(f\"Warning: {fna} empty or missing\")\n",
    "            tocc = time.perf_counter()\n",
    "            print(f\"Total training and evaluation time for group {na}: {tocc - ticc:0.4f} seconds\")\n",
    "\n",
    "print(\"------------------------------------------------------------\")                \n",
    "TOC = time.perf_counter()\n",
    "print(f\"Total training and evaluation time for all groups: {TOC - TIC:0.4f} seconds\")\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(success)>0:\n",
    "    print(f\"{len(success)}/{F} trainings succeeded\")\n",
    "if len(failure)>0:\n",
    "    print(f\"{len(failure)}/{F} trainings failed\")\n",
    "if len(missing)>0:\n",
    "    print(f\"{len(missing)}/{F} trainings without input data\")\n",
    "accounted=len(success)+len(failure)+len(missing)\n",
    "if accounted<F:\n",
    "    print(f\"Warning: {accounted}/{F} trainings cannot be accounted for\")\n",
    "if accounted>F:\n",
    "    print(f\"Warning: {accounted}/{F} trainings accounted for, but only {F} available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_na=\"_\"+model_name+\".\"+config_hash\n",
    "li=[] \n",
    "for channel in l_channel:\n",
    "    for blur in l_blur:\n",
    "        for nfeat in l_nfeat:\n",
    "            group_na=channel+\"_\"+str(nfeat)+\"_\"+str(blur)\n",
    "            fna=group_na+case_na+\".results.csv\"\n",
    "            c=0\n",
    "            if not_missing_or_empty(fna):\n",
    "                c+=1\n",
    "                df=pd.read_csv(fna).reset_index(drop=True)\n",
    "                data=[[channel,nfeat,blur,n] for n in range(df.shape[0])]\n",
    "                df=pd.concat([pd.DataFrame(data, columns=['CHANNEL','NFEAT','BLUR','FOLD']),df],axis=1)              \n",
    "                li.append(df)\n",
    "df = pd.concat(li,ignore_index=True)\n",
    "print(df)\n",
    "df.to_csv('results'+case_na+'.csv',index=0)\n",
    "#\n",
    "fig, ((ax1,ax2,ax3),(ax4,ax5,ax6),(ax7,ax8,ax9)) = plt.subplots(3,3,figsize=(15, 15), sharex=True, sharey=True)\n",
    "fig.suptitle('Accuracy of Random Forrest Classifier')\n",
    "for ax in fig.get_axes():\n",
    "    ax.label_outer()\n",
    "ax1.set_title(\"PR BLUR=0\")\n",
    "sns.lineplot(ax=ax1, data=df.loc[ (df['CHANNEL']=='pr') & (df['BLUR']==0) & (df['FOLD']<10) ],x='NFEAT',hue='FOLD',y='Accuracy',markers=True,)\n",
    "ax2.set_title(\"PR BLUR=4\")\n",
    "sns.lineplot(ax=ax2, data=df.loc[ (df['CHANNEL']=='pr') & (df['BLUR']==4) & (df['FOLD']<10) ],x='NFEAT',hue='FOLD',y='Accuracy',markers=True,)\n",
    "ax3.set_title(\"PR BLUR=12\")\n",
    "sns.lineplot(ax=ax3, data=df.loc[ (df['CHANNEL']=='pr') & (df['BLUR']==12) & (df['FOLD']<10) ],x='NFEAT',hue='FOLD',y='Accuracy',markers=True,)\n",
    "\n",
    "ax4.set_title(\"PW BLUR=0\")\n",
    "sns.lineplot(ax=ax4, data=df.loc[ (df['CHANNEL']=='pw') & (df['BLUR']==0) & (df['FOLD']<10) ],x='NFEAT',hue='FOLD',y='Accuracy',markers=True,)\n",
    "ax5.set_title(\"PW BLUR=4\")\n",
    "sns.lineplot(ax=ax5, data=df.loc[ (df['CHANNEL']=='pw') & (df['BLUR']==4) & (df['FOLD']<10) ],x='NFEAT',hue='FOLD',y='Accuracy',markers=True,)\n",
    "ax6.set_title(\"PW BLUR=12\")\n",
    "sns.lineplot(ax=ax6, data=df.loc[ (df['CHANNEL']=='pw') & (df['BLUR']==12) & (df['FOLD']<10) ],x='NFEAT',hue='FOLD',y='Accuracy',markers=True,)\n",
    "\n",
    "ax7.set_title(\"MA BLUR=0\")\n",
    "sns.lineplot(ax=ax7, data=df.loc[ (df['CHANNEL']=='ma') & (df['BLUR']==0) & (df['FOLD']<10) ],x='NFEAT',hue='FOLD',y='Accuracy',markers=True,)\n",
    "ax8.set_title(\"MA BLUR=4\")\n",
    "sns.lineplot(ax=ax8, data=df.loc[ (df['CHANNEL']=='ma') & (df['BLUR']==4) & (df['FOLD']<10) ],x='NFEAT',hue='FOLD',y='Accuracy',markers=True,)\n",
    "ax9.set_title(\"MA BLUR=12\")\n",
    "sns.lineplot(ax=ax9, data=df.loc[ (df['CHANNEL']=='ma') & (df['BLUR']==12) & (df['FOLD']<10) ],x='NFEAT',hue='FOLD',y='Accuracy',markers=True,)\n",
    "fig.tight_layout()\n",
    "plt.plot()\n",
    "plt.savefig('accuracy_over_nfeat.'+config_hash+'.png')\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5b38e1a51499162b676dc5251a60a01f8fed50c0ea514c66f20c39d10c369bab"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
